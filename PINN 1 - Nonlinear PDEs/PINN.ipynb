{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import pyDOE2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bounds = [-1, 1]\n",
    "t_bounds = [0, 1]\n",
    "num_data_points = 100\n",
    "proportion_t_0 = 0.5 #the proportion of the data points which will exist at various points x along the boundary t = 0. The rest will be split between the boundaries x = -1 and x = 1 for all t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_t_0 = (int) (num_data_points * proportion_t_0)\n",
    "num_points_x_1 = (num_data_points - num_points_t_0)//2 # // is integer division\n",
    "num_points_x_neg_1 = num_data_points - num_points_t_0 - num_points_x_1\n",
    "\n",
    "#create num_data_points random data points on the boundaries of the PDE\n",
    "t_0_points = np.array( list( zip(np.zeros(num_points_t_0), 2 * np.random.rand(num_points_t_0) - 1 ) ) )\n",
    "x_1_points = np.array( list( zip(np.random.rand(num_points_x_1), np.full(num_points_x_1, 1)) ) ) #np.full() takes paramters shape, value. Shape can be a tuple for multidimensional arrays filled with value.\n",
    "x_neg_1_points = np.array( list( zip(np.random.rand(num_points_x_neg_1), np.full(num_points_x_neg_1, -1)) ) )\n",
    "x_points = np.concatenate(( x_1_points, x_neg_1_points ))\n",
    "\n",
    "#Generating labels with the data\n",
    "dtype = [('points', float, 2), ('label', float)] #need custom dtype because otherwise numpy doesn't like these combined arrays\n",
    "\n",
    "t_0_labels = -np.sin(np.pi * t_0_points[:,1] )\n",
    "t_0_combined = np.array(list( zip(t_0_points, t_0_labels) ), dtype=dtype)\n",
    "\n",
    "x_labels = np.zeros(num_points_x_1 + num_points_x_neg_1)\n",
    "x_combined = np.array(list( zip(x_points, x_labels) ), dtype=dtype)\n",
    "\n",
    "combined_labels_data = np.concatenate( (t_0_combined, x_combined) )\n",
    "\n",
    "np.random.shuffle(combined_labels_data)\n",
    "\n",
    "data_points, labels = map(np.array, map(list, zip(*combined_labels_data)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_DataSet(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "trainset = PINN_DataSet(data_points, labels)\n",
    "\n",
    "batch_size = num_data_points #no mini-batches\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhs_samples(n): #generate n collocation points via Latin Hypercube Sampling. Each point is a (t,x)\n",
    "    lhs_array = pyDOE2.lhs(2, samples=n) #Two dimensions. Values from 0 to 1\n",
    "    lhs_array[:,1] = 2*lhs_array[:,1] - 1 #convert range of x values to -1 to 1\n",
    "    return lhs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential( #9 layers of 20 neurons each\n",
    "            nn.Linear(2,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = PINN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters())\n",
    "\n",
    "num_epochs = 300 #I have no idea how many epochs were used in the paper's implementation. Let's just do a lot for now and see how quickly training converges\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-1-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

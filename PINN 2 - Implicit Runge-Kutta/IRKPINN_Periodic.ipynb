{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: I should defintely explore an object-oriented PyTorch approach in the future. It is annoying having to pass around every single required variable to helper functions\n",
    "\n",
    "Attempt to approximate period Sine-Gordon Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io\n",
    "\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd.functional import hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of intermediate steps\n",
    "q = 500\n",
    "\n",
    "#step size\n",
    "h = 0.8\n",
    "\n",
    "#start and end location\n",
    "start_t = 0\n",
    "end_t = start_t + h\n",
    "\n",
    "N = 100 #number of x values for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata = scipy.io.loadmat('./data/burgers_shock.mat')\\n\\nt_points = data['t'].flatten()[:,None] # T x 1\\nx_points = data['x'].flatten()[:,None] # N x 1\\nExact = np.real(data['usol']).T # T x N\\n\\n#get data labels at t = 0.9 \\nindex = np.where(t_points == end_t)[0][0]\\nend_labels = Exact[index, :]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREDIT TO https://github.com/maziarraissi/PINNs/blob/master/appendix/discrete_time_inference%20(Burgers)/Burgers_systematic.py FOR THE BELOW CODE TO IMPORT IRK WEIGHTS AND THE LABEL DATA (AND FOR THE DATA ITSELF)\n",
    "\n",
    "tmp = np.float32(np.loadtxt('./data/Butcher_IRK%d.txt' % (q), ndmin = 2))\n",
    "IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q)) #presume the first q rows and q columns represent a_{ij} values, and the final row represents the b_j values\n",
    "IRK_times = tmp[q**2+q:] #not sure what this is used for\n",
    "\n",
    "\n",
    "#I may choose to generate my own data once more...\n",
    "'''\n",
    "data = scipy.io.loadmat('./data/burgers_shock.mat')\n",
    "\n",
    "t_points = data['t'].flatten()[:,None] # T x 1\n",
    "x_points = data['x'].flatten()[:,None] # N x 1\n",
    "Exact = np.real(data['usol']).T # T x N\n",
    "\n",
    "#get data labels at t = 0.9 \n",
    "index = np.where(t_points == end_t)[0][0]\n",
    "end_labels = Exact[index, :]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = np.random.rand(N) * 2 * np.pi - np.pi #x values between -pi and pi\n",
    "input = np.linspace(-np.pi, np.pi, num=100, endpoint=True)\n",
    "\n",
    "sigma = 1\n",
    "\n",
    "initial_vals =  4*np.arctan(np.exp( -(input**2)/(2*(sigma**2)) )) #initial values at t = 0. u(0, x) = 4arctan(exp(-\\frac{x^2}{2sigma^2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.act_func = nn.Tanh()\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, q + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Nonelinear Burgers' Operator\\ndef NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\\n\\n\\n    u = outputs\\n    print(input.shape)\\n    print(u.shape)\\n\\n    u_x = jacobian(lambda u,x: u, (u, input), create_graph=True, vectorize=True)\\n\\n    print(len(u_x))\\n    print(u_x[0].shape)\\n    print(u_x[1].shape)\\n\\n    u_x = u_x.squeeze(-1) #Jacobian with vectorize should return shape (N, q, 1)\\n\\n    u_xx = jacobian(lambda u_x, x: u_x, (u_x, input), create_graph=True, vectorize=True)\\n\\n    u_xx = u_xx.squeeze(-1)\\n\\n\\n    return u * u_x - (0.01/torch.pi)*u_xx\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Nonelinear Burgers' Operator\n",
    "def NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\n",
    "\n",
    "\n",
    "    u = outputs\n",
    "    print(input.shape)\n",
    "    print(u.shape)\n",
    "\n",
    "    u_x = jacobian(lambda u,x: u, (u, input), create_graph=True, vectorize=True)\n",
    "\n",
    "    print(len(u_x))\n",
    "    print(u_x[0].shape)\n",
    "    print(u_x[1].shape)\n",
    "\n",
    "    u_x = u_x.squeeze(-1) #Jacobian with vectorize should return shape (N, q, 1)\n",
    "\n",
    "    u_xx = jacobian(lambda u_x, x: u_x, (u_x, input), create_graph=True, vectorize=True)\n",
    "\n",
    "    u_xx = u_xx.squeeze(-1)\n",
    "\n",
    "\n",
    "    return u * u_x - (0.01/torch.pi)*u_xx'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "\n",
    "#Nonelinear Burgers' Operator\n",
    "def NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\n",
    "\n",
    "\n",
    "    global N\n",
    "    global q\n",
    "\n",
    "    final_output = torch.empty(N, q, requires_grad=True).to(device)\n",
    "\n",
    "    u = outputs.to(torch.float32)\n",
    "\n",
    "    for i in range(q):\n",
    "\n",
    "        new_u = u[:, i].view(-1, 1).to(torch.float32)\n",
    "\n",
    "        u_x = torch.autograd.grad(new_u, input, grad_outputs=torch.ones_like(new_u), retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, input, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        #print(new_u.shape)\n",
    "        #print(u_x.shape)\n",
    "        #print(u_xx.shape)\n",
    "        #print(final_output[:, i].shape)\n",
    "        new_column = (-u_xx + torch.sin(new_u)).squeeze(-1).to(device)\n",
    "        final_output = torch.cat([final_output[:, :i], new_column.unsqueeze(1), final_output[:, i+1:]], dim=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "\n",
    "    #return outputs\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Implicit Runge-Kutta Calculation (Combine Intermediate Terms with Butcher Tableau Coefficients)\n",
    "def IRK(input, outputs, initial_vals, coefficients, device): #outputs is tensor of size Nx(q+1), represnting q stages for N x-values and the final n+1 prediction. coefficients is a tensor of size (q+1) x q representing the Butcher Tableau. Input and initial_vals are size (N)\n",
    "\n",
    "    outputs_truncated = outputs[:, :-1] #get all but last row\n",
    "\n",
    "    global N\n",
    "    global q\n",
    "    global h\n",
    "\n",
    "\n",
    "    NLO_outputs = NLO(input, outputs_truncated, device) #perform non-linear operation on each of the outputs_truncated. Shape is still (N, q)\n",
    "    \n",
    "    N_coefficients = coefficients.repeat(N, 1, 1) #stack N coefficients tensors on top of each other\n",
    "    NLO_outputs = NLO_outputs.unsqueeze(-1) #reshape NLO_outputs_reshaped into shape (N, q, 1) for matrix multiplication\n",
    "\n",
    "    coefficients_applied = torch.bmm(N_coefficients, NLO_outputs) #perform matrix multiplication. This represents applying the coefficients a_{ij} (and b_i for n+1 prediction) from the Butcher Tableau to every element in NLO_outputs and THEN recombining them to make every new value which will go on to form the \"calculated\" outputs (intermediate stage values). Has shape (N, q+1, 1), since N_coefficients has shape (N, q+1, q) and NLO_outputs has shape (N, q, 1). For each layer in N, the matrix mulitplcaion (q+1, q) x (q, 1) takes place, resulting in a vector of shape (q+1, 1). N of these makes (N, q+1, 1)\n",
    "    coefficients_applied = coefficients_applied.view(N, -1) #now shape (N, q + 1)\n",
    "\n",
    "    coefficients_applied = -h * coefficients_applied #apply the -h (-\\delta t) to each element in coefficients_applied\n",
    "    initial_vals = initial_vals.view(-1, 1) #reshape initial_vals from (N) to (N, 1)\n",
    "\n",
    "    result = coefficients_applied + initial_vals #Every element in coefficients_applied has a value added to it which is the element from initial_vals on the same layer. We can add the shapes (N, q+1) and (N, 1) due to \"broadcasting\", which effectively stretches the shape of the vector\n",
    "\n",
    "    return result #returns a result which is the same shape as \"outputs\" and represents the same intermediate values, except these are the \"calculated\" versions and not the \"direct predictions\"\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "def SSE_n(input, outputs, initial_vals, coefficients, device):\n",
    "\n",
    "    calculated_outputs = IRK(input, outputs, initial_vals, coefficients, device)\n",
    "\n",
    "    SSE_n_loss = torch.sum((calculated_outputs - outputs)**2)\n",
    "\n",
    "    return SSE_n_loss\n",
    "\n",
    "\n",
    "def SSE_b(network, outputs, device):\n",
    "    \n",
    "    half_N_round_down = N // 2\n",
    "\n",
    "    first_half_outputs = outputs[:half_N_round_down]\n",
    "    second_half_outputs = outputs[-half_N_round_down:]\n",
    "    second_half_outputs = torch.flip(second_half_outputs, dims=[0]) #reverse the direction across dim 0\n",
    "\n",
    "    difference = first_half_outputs - second_half_outputs\n",
    "\n",
    "    return torch.sum(difference**2)\n",
    "\n",
    "\n",
    "\n",
    "def SSE(input, outputs, initial_vals, coefficients, network, device):\n",
    "    SSE_n_loss = SSE_n(input, outputs, initial_vals, coefficients, device)\n",
    "    SSE_b_loss = SSE_b(network, outputs, device)\n",
    "    return SSE_n_loss + SSE_b_loss, SSE_n_loss, SSE_b_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "pinn = PINN()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + (\"GPU\" if torch.cuda.is_available() else \"CPU\"))\n",
    "pinn.to(device)\n",
    "\n",
    "\n",
    "input = torch.tensor(input.astype(np.float32), requires_grad=True).to(device).view(-1, 1)\n",
    "initial_vals = torch.tensor(initial_vals.astype(np.float32)).to(device).view(-1, 1)\n",
    "coefficients = torch.tensor(IRK_weights.astype(np.float32)).to(device)\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(pinn.parameters(), #PARAMETERS CREDIT TO https://github.com/teeratornk/PINNs-2/blob/master/Burgers%20Equation/Burgers%20Inference%20(PyTorch).ipynb\n",
    "                              lr=1.0,\n",
    "                              max_iter=50000, \n",
    "                                max_eval=50000, \n",
    "                                history_size=50,\n",
    "                                tolerance_grad=1e-5, \n",
    "                                tolerance_change=1.0 * np.finfo(float).eps,\n",
    "                                line_search_fn=\"strong_wolfe\"\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 | Avg Losses | SSE_n: 158895.0625 | SSE_b: 721.5097045898438 | Total: 159616.578125\n",
      "STEP: 1 | Avg Losses | SSE_n: 158716.9375 | SSE_b: 710.3372192382812 | Total: 159427.28125\n",
      "STEP: 2 | Avg Losses | SSE_n: 157917.0625 | SSE_b: 661.29541015625 | Total: 158578.359375\n",
      "STEP: 3 | Avg Losses | SSE_n: 149584.0 | SSE_b: 266.8204345703125 | Total: 149850.828125\n",
      "STEP: 4 | Avg Losses | SSE_n: 86685.0 | SSE_b: 515.3359375 | Total: 87200.3359375\n",
      "STEP: 5 | Avg Losses | SSE_n: 101697.359375 | SSE_b: 3393.7265625 | Total: 105091.0859375\n",
      "STEP: 6 | Avg Losses | SSE_n: 60006.25390625 | SSE_b: 776.4449462890625 | Total: 60782.69921875\n",
      "STEP: 7 | Avg Losses | SSE_n: 39605.06640625 | SSE_b: 3465.960205078125 | Total: 43071.02734375\n",
      "STEP: 8 | Avg Losses | SSE_n: 114293.5625 | SSE_b: 6864.41845703125 | Total: 121157.984375\n",
      "STEP: 9 | Avg Losses | SSE_n: 29327.08203125 | SSE_b: 5027.220703125 | Total: 34354.3046875\n",
      "STEP: 10 | Avg Losses | SSE_n: 25294.1171875 | SSE_b: 2656.96630859375 | Total: 27951.083984375\n",
      "STEP: 11 | Avg Losses | SSE_n: 17085.60546875 | SSE_b: 2275.783935546875 | Total: 19361.388671875\n",
      "STEP: 12 | Avg Losses | SSE_n: 12185.4375 | SSE_b: 2472.64013671875 | Total: 14658.078125\n",
      "STEP: 13 | Avg Losses | SSE_n: 10392.39453125 | SSE_b: 2303.73681640625 | Total: 12696.130859375\n",
      "STEP: 14 | Avg Losses | SSE_n: 9019.29296875 | SSE_b: 1947.8734130859375 | Total: 10967.166015625\n",
      "STEP: 15 | Avg Losses | SSE_n: 8127.3525390625 | SSE_b: 1915.113525390625 | Total: 10042.4658203125\n",
      "STEP: 16 | Avg Losses | SSE_n: 6500.8447265625 | SSE_b: 1740.2294921875 | Total: 8241.07421875\n",
      "STEP: 17 | Avg Losses | SSE_n: 5034.447265625 | SSE_b: 1433.755859375 | Total: 6468.203125\n",
      "STEP: 18 | Avg Losses | SSE_n: 4430.818359375 | SSE_b: 1198.410400390625 | Total: 5629.228515625\n",
      "STEP: 19 | Avg Losses | SSE_n: 4016.051513671875 | SSE_b: 1151.234375 | Total: 5167.2861328125\n",
      "STEP: 20 | Avg Losses | SSE_n: 3665.89111328125 | SSE_b: 1016.4392700195312 | Total: 4682.33056640625\n",
      "STEP: 21 | Avg Losses | SSE_n: 3418.573486328125 | SSE_b: 883.1717529296875 | Total: 4301.7451171875\n",
      "STEP: 22 | Avg Losses | SSE_n: 2888.90380859375 | SSE_b: 668.407470703125 | Total: 3557.311279296875\n",
      "STEP: 23 | Avg Losses | SSE_n: 2449.6376953125 | SSE_b: 552.1681518554688 | Total: 3001.805908203125\n",
      "STEP: 24 | Avg Losses | SSE_n: 2231.435791015625 | SSE_b: 515.8048706054688 | Total: 2747.24072265625\n",
      "STEP: 25 | Avg Losses | SSE_n: 2032.403564453125 | SSE_b: 485.16510009765625 | Total: 2517.568603515625\n",
      "STEP: 26 | Avg Losses | SSE_n: 1723.564697265625 | SSE_b: 363.7244567871094 | Total: 2087.2890625\n",
      "STEP: 27 | Avg Losses | SSE_n: 1590.12890625 | SSE_b: 285.1108703613281 | Total: 1875.23974609375\n",
      "STEP: 28 | Avg Losses | SSE_n: 1424.3365478515625 | SSE_b: 212.34600830078125 | Total: 1636.6826171875\n",
      "STEP: 29 | Avg Losses | SSE_n: 1290.82421875 | SSE_b: 182.67352294921875 | Total: 1473.497802734375\n",
      "STEP: 30 | Avg Losses | SSE_n: 1223.83984375 | SSE_b: 164.32850646972656 | Total: 1388.1683349609375\n",
      "STEP: 31 | Avg Losses | SSE_n: 1160.6534423828125 | SSE_b: 137.27378845214844 | Total: 1297.92724609375\n",
      "STEP: 32 | Avg Losses | SSE_n: 1081.574951171875 | SSE_b: 113.11060333251953 | Total: 1194.685546875\n",
      "STEP: 33 | Avg Losses | SSE_n: 947.9022827148438 | SSE_b: 94.90360260009766 | Total: 1042.805908203125\n",
      "STEP: 34 | Avg Losses | SSE_n: 838.1414794921875 | SSE_b: 105.57710266113281 | Total: 943.7185668945312\n",
      "STEP: 35 | Avg Losses | SSE_n: 763.931640625 | SSE_b: 97.39778900146484 | Total: 861.3294067382812\n",
      "STEP: 36 | Avg Losses | SSE_n: 685.0363159179688 | SSE_b: 87.23016357421875 | Total: 772.2664794921875\n",
      "STEP: 37 | Avg Losses | SSE_n: 628.0220336914062 | SSE_b: 79.66671752929688 | Total: 707.688720703125\n",
      "STEP: 38 | Avg Losses | SSE_n: 511.5028076171875 | SSE_b: 69.1158447265625 | Total: 580.61865234375\n",
      "STEP: 39 | Avg Losses | SSE_n: 450.3941650390625 | SSE_b: 64.60729217529297 | Total: 515.00146484375\n",
      "STEP: 40 | Avg Losses | SSE_n: 412.73760986328125 | SSE_b: 59.556304931640625 | Total: 472.2939147949219\n",
      "STEP: 41 | Avg Losses | SSE_n: 378.00592041015625 | SSE_b: 51.99171829223633 | Total: 429.9976501464844\n",
      "STEP: 42 | Avg Losses | SSE_n: 332.982421875 | SSE_b: 45.8199577331543 | Total: 378.8023681640625\n",
      "STEP: 43 | Avg Losses | SSE_n: 289.30047607421875 | SSE_b: 45.474979400634766 | Total: 334.77545166015625\n",
      "STEP: 44 | Avg Losses | SSE_n: 248.99264526367188 | SSE_b: 44.77561950683594 | Total: 293.76824951171875\n",
      "STEP: 45 | Avg Losses | SSE_n: 226.75408935546875 | SSE_b: 39.397342681884766 | Total: 266.15142822265625\n",
      "STEP: 46 | Avg Losses | SSE_n: 203.1954803466797 | SSE_b: 33.66828918457031 | Total: 236.86376953125\n",
      "STEP: 47 | Avg Losses | SSE_n: 174.95773315429688 | SSE_b: 28.875518798828125 | Total: 203.833251953125\n",
      "STEP: 48 | Avg Losses | SSE_n: 155.25027465820312 | SSE_b: 27.054649353027344 | Total: 182.304931640625\n",
      "STEP: 49 | Avg Losses | SSE_n: 145.43325805664062 | SSE_b: 25.262920379638672 | Total: 170.69618225097656\n",
      "STEP: 50 | Avg Losses | SSE_n: 138.57241821289062 | SSE_b: 23.063098907470703 | Total: 161.63551330566406\n",
      "STEP: 51 | Avg Losses | SSE_n: 128.52032470703125 | SSE_b: 20.78005599975586 | Total: 149.30038452148438\n",
      "STEP: 52 | Avg Losses | SSE_n: 117.44915771484375 | SSE_b: 19.12750244140625 | Total: 136.57666015625\n",
      "STEP: 53 | Avg Losses | SSE_n: 112.92588806152344 | SSE_b: 18.65674591064453 | Total: 131.5826416015625\n",
      "STEP: 54 | Avg Losses | SSE_n: 110.30857849121094 | SSE_b: 18.328889846801758 | Total: 128.63746643066406\n",
      "STEP: 55 | Avg Losses | SSE_n: 103.87791442871094 | SSE_b: 16.90517807006836 | Total: 120.78309631347656\n",
      "STEP: 56 | Avg Losses | SSE_n: 97.80609130859375 | SSE_b: 16.040019989013672 | Total: 113.84611511230469\n",
      "STEP: 57 | Avg Losses | SSE_n: 95.31784057617188 | SSE_b: 15.083603858947754 | Total: 110.40144348144531\n",
      "STEP: 58 | Avg Losses | SSE_n: 92.64317321777344 | SSE_b: 14.863990783691406 | Total: 107.50716400146484\n",
      "STEP: 59 | Avg Losses | SSE_n: 90.34001159667969 | SSE_b: 14.391678810119629 | Total: 104.731689453125\n",
      "STEP: 60 | Avg Losses | SSE_n: 88.50697326660156 | SSE_b: 13.997986793518066 | Total: 102.50495910644531\n",
      "STEP: 61 | Avg Losses | SSE_n: 85.93406677246094 | SSE_b: 13.46911907196045 | Total: 99.40318298339844\n",
      "STEP: 62 | Avg Losses | SSE_n: 82.50218200683594 | SSE_b: 12.993887901306152 | Total: 95.4960708618164\n",
      "STEP: 63 | Avg Losses | SSE_n: 78.03129577636719 | SSE_b: 12.229004859924316 | Total: 90.26029968261719\n",
      "STEP: 64 | Avg Losses | SSE_n: 75.43441009521484 | SSE_b: 11.895367622375488 | Total: 87.32978057861328\n",
      "STEP: 65 | Avg Losses | SSE_n: 73.1570816040039 | SSE_b: 11.319196701049805 | Total: 84.47628021240234\n",
      "STEP: 66 | Avg Losses | SSE_n: 71.14412689208984 | SSE_b: 10.894560813903809 | Total: 82.03868865966797\n",
      "STEP: 67 | Avg Losses | SSE_n: 69.54911041259766 | SSE_b: 10.220511436462402 | Total: 79.76962280273438\n",
      "STEP: 68 | Avg Losses | SSE_n: 67.73314666748047 | SSE_b: 10.19616985321045 | Total: 77.92931365966797\n",
      "STEP: 69 | Avg Losses | SSE_n: 66.75726318359375 | SSE_b: 9.940436363220215 | Total: 76.69770050048828\n",
      "STEP: 70 | Avg Losses | SSE_n: 65.15470886230469 | SSE_b: 9.880267143249512 | Total: 75.03497314453125\n",
      "STEP: 71 | Avg Losses | SSE_n: 62.506649017333984 | SSE_b: 9.475855827331543 | Total: 71.98250579833984\n",
      "STEP: 72 | Avg Losses | SSE_n: 59.497413635253906 | SSE_b: 9.900063514709473 | Total: 69.39747619628906\n",
      "STEP: 73 | Avg Losses | SSE_n: 58.178504943847656 | SSE_b: 8.954873085021973 | Total: 67.13337707519531\n",
      "STEP: 74 | Avg Losses | SSE_n: 56.843109130859375 | SSE_b: 8.309985160827637 | Total: 65.15309143066406\n",
      "STEP: 75 | Avg Losses | SSE_n: 55.23480987548828 | SSE_b: 7.889997482299805 | Total: 63.12480926513672\n",
      "STEP: 76 | Avg Losses | SSE_n: 52.861000061035156 | SSE_b: 7.830718994140625 | Total: 60.69171905517578\n",
      "STEP: 77 | Avg Losses | SSE_n: 51.842281341552734 | SSE_b: 7.799249649047852 | Total: 59.64153289794922\n",
      "STEP: 78 | Avg Losses | SSE_n: 50.64535903930664 | SSE_b: 7.782515048980713 | Total: 58.42787551879883\n",
      "STEP: 79 | Avg Losses | SSE_n: 49.00828552246094 | SSE_b: 7.592735767364502 | Total: 56.60102081298828\n",
      "STEP: 80 | Avg Losses | SSE_n: 47.74340057373047 | SSE_b: 6.811540603637695 | Total: 54.55493927001953\n",
      "STEP: 81 | Avg Losses | SSE_n: 46.23646545410156 | SSE_b: 6.703554153442383 | Total: 52.94001770019531\n",
      "STEP: 82 | Avg Losses | SSE_n: 45.61894226074219 | SSE_b: 6.413625240325928 | Total: 52.03256607055664\n",
      "STEP: 83 | Avg Losses | SSE_n: 44.789794921875 | SSE_b: 6.243296146392822 | Total: 51.0330924987793\n",
      "STEP: 84 | Avg Losses | SSE_n: 43.58769607543945 | SSE_b: 6.22026252746582 | Total: 49.807960510253906\n",
      "STEP: 85 | Avg Losses | SSE_n: 42.59147262573242 | SSE_b: 5.998544692993164 | Total: 48.59001922607422\n",
      "STEP: 86 | Avg Losses | SSE_n: 41.16066360473633 | SSE_b: 6.046130180358887 | Total: 47.20679473876953\n",
      "STEP: 87 | Avg Losses | SSE_n: 40.504913330078125 | SSE_b: 5.876002788543701 | Total: 46.380916595458984\n",
      "STEP: 88 | Avg Losses | SSE_n: 39.310237884521484 | SSE_b: 5.963753700256348 | Total: 45.273990631103516\n",
      "STEP: 89 | Avg Losses | SSE_n: 38.62397766113281 | SSE_b: 5.694580554962158 | Total: 44.31855773925781\n",
      "STEP: 90 | Avg Losses | SSE_n: 37.81647872924805 | SSE_b: 5.647394180297852 | Total: 43.46387481689453\n",
      "STEP: 91 | Avg Losses | SSE_n: 36.78568649291992 | SSE_b: 5.589323997497559 | Total: 42.3750114440918\n",
      "STEP: 92 | Avg Losses | SSE_n: 35.31208419799805 | SSE_b: 5.476646900177002 | Total: 40.78873062133789\n",
      "STEP: 93 | Avg Losses | SSE_n: 34.28192138671875 | SSE_b: 5.276247978210449 | Total: 39.558170318603516\n",
      "STEP: 94 | Avg Losses | SSE_n: 33.705963134765625 | SSE_b: 5.1425042152404785 | Total: 38.84846878051758\n",
      "STEP: 95 | Avg Losses | SSE_n: 33.193538665771484 | SSE_b: 5.009917736053467 | Total: 38.20345687866211\n",
      "STEP: 96 | Avg Losses | SSE_n: 32.62004470825195 | SSE_b: 4.856355667114258 | Total: 37.476402282714844\n",
      "STEP: 97 | Avg Losses | SSE_n: 31.975139617919922 | SSE_b: 4.665778160095215 | Total: 36.64091873168945\n",
      "STEP: 98 | Avg Losses | SSE_n: 31.44729232788086 | SSE_b: 4.606568336486816 | Total: 36.05385971069336\n",
      "STEP: 99 | Avg Losses | SSE_n: 30.912792205810547 | SSE_b: 4.602206230163574 | Total: 35.51499938964844\n",
      "STEP: 100 | Avg Losses | SSE_n: 30.50566864013672 | SSE_b: 4.545241355895996 | Total: 35.05091094970703\n",
      "STEP: 101 | Avg Losses | SSE_n: 29.795133590698242 | SSE_b: 4.5543437004089355 | Total: 34.3494758605957\n",
      "STEP: 102 | Avg Losses | SSE_n: 29.297832489013672 | SSE_b: 4.4453864097595215 | Total: 33.74321746826172\n",
      "STEP: 103 | Avg Losses | SSE_n: 28.92367172241211 | SSE_b: 4.338297367095947 | Total: 33.26197052001953\n",
      "STEP: 104 | Avg Losses | SSE_n: 28.596860885620117 | SSE_b: 4.265966892242432 | Total: 32.86282730102539\n",
      "STEP: 105 | Avg Losses | SSE_n: 28.279949188232422 | SSE_b: 4.07338809967041 | Total: 32.353336334228516\n",
      "STEP: 106 | Avg Losses | SSE_n: 27.772939682006836 | SSE_b: 3.944040298461914 | Total: 31.71697998046875\n",
      "STEP: 107 | Avg Losses | SSE_n: 27.464582443237305 | SSE_b: 3.761409044265747 | Total: 31.22599220275879\n",
      "STEP: 108 | Avg Losses | SSE_n: 27.09796142578125 | SSE_b: 3.7205023765563965 | Total: 30.818464279174805\n",
      "STEP: 109 | Avg Losses | SSE_n: 26.61981964111328 | SSE_b: 3.70805025100708 | Total: 30.327869415283203\n",
      "STEP: 110 | Avg Losses | SSE_n: 25.863609313964844 | SSE_b: 3.6427619457244873 | Total: 29.506370544433594\n",
      "STEP: 111 | Avg Losses | SSE_n: 25.354480743408203 | SSE_b: 3.565178871154785 | Total: 28.919658660888672\n",
      "STEP: 112 | Avg Losses | SSE_n: 24.782390594482422 | SSE_b: 3.468520402908325 | Total: 28.250911712646484\n",
      "STEP: 113 | Avg Losses | SSE_n: 24.072328567504883 | SSE_b: 3.571852445602417 | Total: 27.644180297851562\n",
      "STEP: 114 | Avg Losses | SSE_n: 23.681594848632812 | SSE_b: 3.476360559463501 | Total: 27.157955169677734\n",
      "STEP: 115 | Avg Losses | SSE_n: 23.3363037109375 | SSE_b: 3.433738946914673 | Total: 26.770042419433594\n",
      "STEP: 116 | Avg Losses | SSE_n: 23.04094696044922 | SSE_b: 3.4139068126678467 | Total: 26.454853057861328\n",
      "STEP: 117 | Avg Losses | SSE_n: 22.420482635498047 | SSE_b: 3.444187879562378 | Total: 25.864669799804688\n",
      "STEP: 118 | Avg Losses | SSE_n: 22.007707595825195 | SSE_b: 3.4788334369659424 | Total: 25.486541748046875\n",
      "STEP: 119 | Avg Losses | SSE_n: 21.662460327148438 | SSE_b: 3.486581802368164 | Total: 25.1490421295166\n",
      "STEP: 120 | Avg Losses | SSE_n: 21.006633758544922 | SSE_b: 3.426870107650757 | Total: 24.433504104614258\n",
      "STEP: 121 | Avg Losses | SSE_n: 20.233455657958984 | SSE_b: 3.1866953372955322 | Total: 23.420150756835938\n",
      "STEP: 122 | Avg Losses | SSE_n: 19.704742431640625 | SSE_b: 3.0335569381713867 | Total: 22.738300323486328\n",
      "STEP: 123 | Avg Losses | SSE_n: 19.207775115966797 | SSE_b: 3.0546984672546387 | Total: 22.262474060058594\n",
      "STEP: 124 | Avg Losses | SSE_n: 18.838607788085938 | SSE_b: 3.0408759117126465 | Total: 21.879484176635742\n",
      "STEP: 125 | Avg Losses | SSE_n: 18.517446517944336 | SSE_b: 2.939920663833618 | Total: 21.457366943359375\n",
      "STEP: 126 | Avg Losses | SSE_n: 17.94074058532715 | SSE_b: 2.9362125396728516 | Total: 20.876953125\n",
      "STEP: 127 | Avg Losses | SSE_n: 17.618324279785156 | SSE_b: 2.814115047454834 | Total: 20.43243980407715\n",
      "STEP: 128 | Avg Losses | SSE_n: 17.41579818725586 | SSE_b: 2.6746792793273926 | Total: 20.090476989746094\n",
      "STEP: 129 | Avg Losses | SSE_n: 17.234451293945312 | SSE_b: 2.628147602081299 | Total: 19.862598419189453\n",
      "STEP: 130 | Avg Losses | SSE_n: 17.063617706298828 | SSE_b: 2.536595106124878 | Total: 19.60021209716797\n",
      "STEP: 131 | Avg Losses | SSE_n: 16.517234802246094 | SSE_b: 2.4617233276367188 | Total: 18.978958129882812\n",
      "STEP: 132 | Avg Losses | SSE_n: 16.002037048339844 | SSE_b: 2.2347092628479004 | Total: 18.236745834350586\n",
      "STEP: 133 | Avg Losses | SSE_n: 15.472972869873047 | SSE_b: 2.2884461879730225 | Total: 17.76141929626465\n",
      "STEP: 134 | Avg Losses | SSE_n: 15.16856575012207 | SSE_b: 2.2723469734191895 | Total: 17.4409122467041\n",
      "STEP: 135 | Avg Losses | SSE_n: 14.625919342041016 | SSE_b: 2.213315486907959 | Total: 16.839235305786133\n",
      "STEP: 136 | Avg Losses | SSE_n: 14.031993865966797 | SSE_b: 2.254981756210327 | Total: 16.286975860595703\n",
      "STEP: 137 | Avg Losses | SSE_n: 13.756406784057617 | SSE_b: 2.122209072113037 | Total: 15.878616333007812\n",
      "STEP: 138 | Avg Losses | SSE_n: 13.52511978149414 | SSE_b: 2.1155264377593994 | Total: 15.640645980834961\n",
      "STEP: 139 | Avg Losses | SSE_n: 13.266683578491211 | SSE_b: 2.076172351837158 | Total: 15.342855453491211\n",
      "STEP: 140 | Avg Losses | SSE_n: 12.926446914672852 | SSE_b: 2.041170597076416 | Total: 14.96761703491211\n",
      "STEP: 141 | Avg Losses | SSE_n: 12.489654541015625 | SSE_b: 1.9692120552062988 | Total: 14.458866119384766\n",
      "STEP: 142 | Avg Losses | SSE_n: 12.165803909301758 | SSE_b: 1.9670637845993042 | Total: 14.132867813110352\n",
      "STEP: 143 | Avg Losses | SSE_n: 11.92646598815918 | SSE_b: 1.9006109237670898 | Total: 13.82707691192627\n",
      "STEP: 144 | Avg Losses | SSE_n: 11.550085067749023 | SSE_b: 1.7628273963928223 | Total: 13.312911987304688\n",
      "STEP: 145 | Avg Losses | SSE_n: 11.297155380249023 | SSE_b: 1.7120977640151978 | Total: 13.00925350189209\n",
      "STEP: 146 | Avg Losses | SSE_n: 11.144855499267578 | SSE_b: 1.6912932395935059 | Total: 12.836149215698242\n",
      "STEP: 147 | Avg Losses | SSE_n: 11.035778045654297 | SSE_b: 1.6728768348693848 | Total: 12.708654403686523\n",
      "STEP: 148 | Avg Losses | SSE_n: 10.766969680786133 | SSE_b: 1.6130752563476562 | Total: 12.380044937133789\n",
      "STEP: 149 | Avg Losses | SSE_n: 10.456028938293457 | SSE_b: 1.5530496835708618 | Total: 12.009078979492188\n",
      "STEP: 150 | Avg Losses | SSE_n: 10.156712532043457 | SSE_b: 1.5283359289169312 | Total: 11.68504810333252\n",
      "STEP: 151 | Avg Losses | SSE_n: 10.09676456451416 | SSE_b: 1.4968454837799072 | Total: 11.593609809875488\n",
      "STEP: 152 | Avg Losses | SSE_n: 9.9727783203125 | SSE_b: 1.4717233180999756 | Total: 11.444501876831055\n",
      "STEP: 153 | Avg Losses | SSE_n: 9.771263122558594 | SSE_b: 1.442194938659668 | Total: 11.213458061218262\n",
      "STEP: 154 | Avg Losses | SSE_n: 9.534173011779785 | SSE_b: 1.4615756273269653 | Total: 10.995748519897461\n",
      "STEP: 155 | Avg Losses | SSE_n: 9.43116569519043 | SSE_b: 1.430726408958435 | Total: 10.861891746520996\n",
      "STEP: 156 | Avg Losses | SSE_n: 9.332744598388672 | SSE_b: 1.3921058177947998 | Total: 10.72485065460205\n",
      "STEP: 157 | Avg Losses | SSE_n: 9.182769775390625 | SSE_b: 1.3534185886383057 | Total: 10.536188125610352\n",
      "STEP: 158 | Avg Losses | SSE_n: 8.932750701904297 | SSE_b: 1.315016746520996 | Total: 10.247767448425293\n",
      "STEP: 159 | Avg Losses | SSE_n: 9.166711807250977 | SSE_b: 1.2953988313674927 | Total: 10.46211051940918\n",
      "STEP: 160 | Avg Losses | SSE_n: 8.887542724609375 | SSE_b: 1.2866061925888062 | Total: 10.174148559570312\n",
      "STEP: 161 | Avg Losses | SSE_n: 8.761550903320312 | SSE_b: 1.2899903059005737 | Total: 10.051541328430176\n",
      "STEP: 162 | Avg Losses | SSE_n: 8.640844345092773 | SSE_b: 1.2717443704605103 | Total: 9.912589073181152\n",
      "STEP: 163 | Avg Losses | SSE_n: 8.5517578125 | SSE_b: 1.2625441551208496 | Total: 9.814302444458008\n",
      "STEP: 164 | Avg Losses | SSE_n: 8.400588989257812 | SSE_b: 1.2346184253692627 | Total: 9.635207176208496\n",
      "STEP: 165 | Avg Losses | SSE_n: 8.13531494140625 | SSE_b: 1.2070424556732178 | Total: 9.342357635498047\n",
      "STEP: 166 | Avg Losses | SSE_n: 7.9424591064453125 | SSE_b: 1.2079739570617676 | Total: 9.150432586669922\n",
      "STEP: 167 | Avg Losses | SSE_n: 7.769975185394287 | SSE_b: 1.1670663356781006 | Total: 8.937041282653809\n",
      "STEP: 168 | Avg Losses | SSE_n: 7.6464715003967285 | SSE_b: 1.1428377628326416 | Total: 8.78930950164795\n",
      "STEP: 169 | Avg Losses | SSE_n: 7.510199546813965 | SSE_b: 1.1240675449371338 | Total: 8.63426685333252\n",
      "STEP: 170 | Avg Losses | SSE_n: 7.234899520874023 | SSE_b: 1.0902316570281982 | Total: 8.3251314163208\n",
      "STEP: 171 | Avg Losses | SSE_n: 7.0267252922058105 | SSE_b: 1.0769307613372803 | Total: 8.103655815124512\n",
      "STEP: 172 | Avg Losses | SSE_n: 6.875946044921875 | SSE_b: 1.0822972059249878 | Total: 7.958243370056152\n",
      "STEP: 173 | Avg Losses | SSE_n: 6.801263332366943 | SSE_b: 1.073230266571045 | Total: 7.874493598937988\n",
      "STEP: 174 | Avg Losses | SSE_n: 6.668843746185303 | SSE_b: 1.0609320402145386 | Total: 7.729775905609131\n",
      "STEP: 175 | Avg Losses | SSE_n: 6.489550590515137 | SSE_b: 1.0276416540145874 | Total: 7.517192363739014\n",
      "STEP: 176 | Avg Losses | SSE_n: 6.327878475189209 | SSE_b: 1.0370599031448364 | Total: 7.364938259124756\n",
      "STEP: 177 | Avg Losses | SSE_n: 6.168112754821777 | SSE_b: 1.0234062671661377 | Total: 7.191518783569336\n",
      "STEP: 178 | Avg Losses | SSE_n: 6.093216896057129 | SSE_b: 0.9788903594017029 | Total: 7.072107315063477\n",
      "STEP: 179 | Avg Losses | SSE_n: 5.964797019958496 | SSE_b: 0.9849972128868103 | Total: 6.949794292449951\n",
      "STEP: 180 | Avg Losses | SSE_n: 5.836316108703613 | SSE_b: 0.9862728118896484 | Total: 6.822588920593262\n",
      "STEP: 181 | Avg Losses | SSE_n: 5.730741024017334 | SSE_b: 0.9798701405525208 | Total: 6.710611343383789\n",
      "STEP: 182 | Avg Losses | SSE_n: 5.641235828399658 | SSE_b: 0.9688642024993896 | Total: 6.610099792480469\n",
      "STEP: 183 | Avg Losses | SSE_n: 5.510586738586426 | SSE_b: 0.9619656801223755 | Total: 6.472552299499512\n",
      "STEP: 184 | Avg Losses | SSE_n: 5.3579254150390625 | SSE_b: 0.9648758172988892 | Total: 6.322801113128662\n",
      "STEP: 185 | Avg Losses | SSE_n: 5.2703633308410645 | SSE_b: 0.946475625038147 | Total: 6.216838836669922\n",
      "STEP: 186 | Avg Losses | SSE_n: 5.29107666015625 | SSE_b: 0.9702825546264648 | Total: 6.261359214782715\n",
      "STEP: 187 | Avg Losses | SSE_n: 5.204075813293457 | SSE_b: 0.9404143691062927 | Total: 6.1444902420043945\n",
      "STEP: 188 | Avg Losses | SSE_n: 5.140031814575195 | SSE_b: 0.9176160097122192 | Total: 6.057647705078125\n",
      "STEP: 189 | Avg Losses | SSE_n: 5.031417369842529 | SSE_b: 0.9026884436607361 | Total: 5.93410587310791\n",
      "STEP: 190 | Avg Losses | SSE_n: 4.937185287475586 | SSE_b: 0.8928651809692383 | Total: 5.830050468444824\n",
      "STEP: 191 | Avg Losses | SSE_n: 4.8448591232299805 | SSE_b: 0.8772484064102173 | Total: 5.722107410430908\n",
      "STEP: 192 | Avg Losses | SSE_n: 4.737316131591797 | SSE_b: 0.9074887037277222 | Total: 5.644804954528809\n",
      "STEP: 193 | Avg Losses | SSE_n: 4.670622825622559 | SSE_b: 0.8612849712371826 | Total: 5.53190803527832\n",
      "STEP: 194 | Avg Losses | SSE_n: 4.597548484802246 | SSE_b: 0.8501806855201721 | Total: 5.447729110717773\n",
      "STEP: 195 | Avg Losses | SSE_n: 4.528773784637451 | SSE_b: 0.8392163515090942 | Total: 5.367990016937256\n",
      "STEP: 196 | Avg Losses | SSE_n: 4.39840841293335 | SSE_b: 0.8237423896789551 | Total: 5.222150802612305\n",
      "STEP: 197 | Avg Losses | SSE_n: 4.325125217437744 | SSE_b: 0.8111714124679565 | Total: 5.13629674911499\n",
      "STEP: 198 | Avg Losses | SSE_n: 4.203142166137695 | SSE_b: 0.7918240427970886 | Total: 4.99496603012085\n",
      "STEP: 199 | Avg Losses | SSE_n: 4.139318943023682 | SSE_b: 0.7834340333938599 | Total: 4.922752857208252\n",
      "STEP: 200 | Avg Losses | SSE_n: 4.075051784515381 | SSE_b: 0.7873147130012512 | Total: 4.862366676330566\n",
      "STEP: 201 | Avg Losses | SSE_n: 4.035634994506836 | SSE_b: 0.7781453132629395 | Total: 4.813780307769775\n",
      "STEP: 202 | Avg Losses | SSE_n: 3.9487602710723877 | SSE_b: 0.7528132200241089 | Total: 4.701573371887207\n",
      "STEP: 203 | Avg Losses | SSE_n: 3.8790063858032227 | SSE_b: 0.7296727895736694 | Total: 4.608679294586182\n",
      "STEP: 204 | Avg Losses | SSE_n: 3.8028037548065186 | SSE_b: 0.7219103574752808 | Total: 4.52471399307251\n",
      "STEP: 205 | Avg Losses | SSE_n: 3.771745443344116 | SSE_b: 0.7150545120239258 | Total: 4.486800193786621\n",
      "STEP: 206 | Avg Losses | SSE_n: 3.70505952835083 | SSE_b: 0.7011957764625549 | Total: 4.40625524520874\n",
      "STEP: 207 | Avg Losses | SSE_n: 3.632509231567383 | SSE_b: 0.6822653412818909 | Total: 4.314774513244629\n",
      "STEP: 208 | Avg Losses | SSE_n: 3.8025035858154297 | SSE_b: 0.6866558194160461 | Total: 4.48915958404541\n",
      "STEP: 209 | Avg Losses | SSE_n: 3.5964102745056152 | SSE_b: 0.6708402037620544 | Total: 4.2672505378723145\n",
      "STEP: 210 | Avg Losses | SSE_n: 3.543351650238037 | SSE_b: 0.6558743119239807 | Total: 4.199225902557373\n",
      "STEP: 211 | Avg Losses | SSE_n: 3.5060997009277344 | SSE_b: 0.6495963335037231 | Total: 4.155695915222168\n",
      "STEP: 212 | Avg Losses | SSE_n: 3.4681098461151123 | SSE_b: 0.6426692605018616 | Total: 4.110779285430908\n",
      "STEP: 213 | Avg Losses | SSE_n: 3.3903186321258545 | SSE_b: 0.634003221988678 | Total: 4.024322032928467\n",
      "STEP: 214 | Avg Losses | SSE_n: 3.3551387786865234 | SSE_b: 0.6312617659568787 | Total: 3.986400604248047\n",
      "STEP: 215 | Avg Losses | SSE_n: 3.3093721866607666 | SSE_b: 0.618575394153595 | Total: 3.927947521209717\n",
      "STEP: 216 | Avg Losses | SSE_n: 3.286195993423462 | SSE_b: 0.6128988265991211 | Total: 3.899094820022583\n",
      "STEP: 217 | Avg Losses | SSE_n: 3.2677271366119385 | SSE_b: 0.6071808338165283 | Total: 3.874907970428467\n",
      "STEP: 218 | Avg Losses | SSE_n: 3.2258172035217285 | SSE_b: 0.6035773158073425 | Total: 3.829394578933716\n",
      "STEP: 219 | Avg Losses | SSE_n: 3.1724302768707275 | SSE_b: 0.5952143669128418 | Total: 3.7676446437835693\n",
      "STEP: 220 | Avg Losses | SSE_n: 3.13511323928833 | SSE_b: 0.5941548943519592 | Total: 3.7292680740356445\n",
      "STEP: 221 | Avg Losses | SSE_n: 3.1118948459625244 | SSE_b: 0.584861159324646 | Total: 3.696755886077881\n",
      "STEP: 222 | Avg Losses | SSE_n: 3.073151111602783 | SSE_b: 0.5746669173240662 | Total: 3.647818088531494\n",
      "STEP: 223 | Avg Losses | SSE_n: 3.032564163208008 | SSE_b: 0.5664248466491699 | Total: 3.5989890098571777\n",
      "STEP: 224 | Avg Losses | SSE_n: 2.9891247749328613 | SSE_b: 0.5538465976715088 | Total: 3.54297137260437\n",
      "STEP: 225 | Avg Losses | SSE_n: 2.9593348503112793 | SSE_b: 0.5564591288566589 | Total: 3.515794038772583\n",
      "STEP: 226 | Avg Losses | SSE_n: 2.9236483573913574 | SSE_b: 0.5373802185058594 | Total: 3.461028575897217\n",
      "STEP: 227 | Avg Losses | SSE_n: 2.8985185623168945 | SSE_b: 0.5298560857772827 | Total: 3.428374767303467\n",
      "STEP: 228 | Avg Losses | SSE_n: 2.862114429473877 | SSE_b: 0.5230506062507629 | Total: 3.385164976119995\n",
      "STEP: 229 | Avg Losses | SSE_n: 2.798856735229492 | SSE_b: 0.5073333978652954 | Total: 3.306190013885498\n",
      "STEP: 230 | Avg Losses | SSE_n: 2.741145610809326 | SSE_b: 0.49368590116500854 | Total: 3.2348315715789795\n",
      "STEP: 231 | Avg Losses | SSE_n: 2.749420166015625 | SSE_b: 0.4903067946434021 | Total: 3.239727020263672\n",
      "STEP: 232 | Avg Losses | SSE_n: 2.705502986907959 | SSE_b: 0.4797859191894531 | Total: 3.185288906097412\n",
      "STEP: 233 | Avg Losses | SSE_n: 2.6732640266418457 | SSE_b: 0.46817970275878906 | Total: 3.1414437294006348\n",
      "STEP: 234 | Avg Losses | SSE_n: 2.6500139236450195 | SSE_b: 0.46389734745025635 | Total: 3.1139111518859863\n",
      "STEP: 235 | Avg Losses | SSE_n: 2.6293067932128906 | SSE_b: 0.4561392664909363 | Total: 3.0854461193084717\n",
      "STEP: 236 | Avg Losses | SSE_n: 2.5787289142608643 | SSE_b: 0.4453011751174927 | Total: 3.0240302085876465\n",
      "STEP: 237 | Avg Losses | SSE_n: 2.54653263092041 | SSE_b: 0.42835283279418945 | Total: 2.9748854637145996\n",
      "STEP: 238 | Avg Losses | SSE_n: 2.5025031566619873 | SSE_b: 0.42137831449508667 | Total: 2.9238815307617188\n",
      "STEP: 239 | Avg Losses | SSE_n: 2.460003137588501 | SSE_b: 0.41779664158821106 | Total: 2.8777997493743896\n",
      "STEP: 240 | Avg Losses | SSE_n: 2.4500718116760254 | SSE_b: 0.4172988831996918 | Total: 2.86737060546875\n",
      "STEP: 241 | Avg Losses | SSE_n: 2.429990291595459 | SSE_b: 0.4128096401691437 | Total: 2.8427999019622803\n",
      "STEP: 242 | Avg Losses | SSE_n: 2.4184422492980957 | SSE_b: 0.41008204221725464 | Total: 2.828524351119995\n",
      "STEP: 243 | Avg Losses | SSE_n: 2.4082679748535156 | SSE_b: 0.40632548928260803 | Total: 2.814593553543091\n",
      "STEP: 244 | Avg Losses | SSE_n: 2.391702175140381 | SSE_b: 0.4007546305656433 | Total: 2.792456865310669\n",
      "STEP: 245 | Avg Losses | SSE_n: 2.378124237060547 | SSE_b: 0.3937184810638428 | Total: 2.7718427181243896\n",
      "STEP: 246 | Avg Losses | SSE_n: 2.3552513122558594 | SSE_b: 0.39207345247268677 | Total: 2.7473247051239014\n",
      "STEP: 247 | Avg Losses | SSE_n: 2.3475284576416016 | SSE_b: 0.3898451030254364 | Total: 2.7373735904693604\n",
      "STEP: 248 | Avg Losses | SSE_n: 2.3350067138671875 | SSE_b: 0.3871932029724121 | Total: 2.7221999168395996\n",
      "STEP: 249 | Avg Losses | SSE_n: 2.3311567306518555 | SSE_b: 0.37729746103286743 | Total: 2.708454132080078\n",
      "STEP: 250 | Avg Losses | SSE_n: 2.3071846961975098 | SSE_b: 0.37290340662002563 | Total: 2.6800880432128906\n",
      "STEP: 251 | Avg Losses | SSE_n: 2.296520233154297 | SSE_b: 0.3684953451156616 | Total: 2.665015697479248\n",
      "STEP: 252 | Avg Losses | SSE_n: 2.287360668182373 | SSE_b: 0.3632017970085144 | Total: 2.6505625247955322\n",
      "STEP: 253 | Avg Losses | SSE_n: 2.272998332977295 | SSE_b: 0.3561505079269409 | Total: 2.6291489601135254\n",
      "STEP: 254 | Avg Losses | SSE_n: 2.254056453704834 | SSE_b: 0.34752345085144043 | Total: 2.6015799045562744\n",
      "STEP: 255 | Avg Losses | SSE_n: 2.2423453330993652 | SSE_b: 0.3447513282299042 | Total: 2.587096691131592\n",
      "STEP: 256 | Avg Losses | SSE_n: 2.2280678749084473 | SSE_b: 0.3422870934009552 | Total: 2.57035493850708\n",
      "STEP: 257 | Avg Losses | SSE_n: 2.206510543823242 | SSE_b: 0.33846962451934814 | Total: 2.544980049133301\n",
      "STEP: 258 | Avg Losses | SSE_n: 2.293759822845459 | SSE_b: 0.3745637536048889 | Total: 2.668323516845703\n",
      "STEP: 259 | Avg Losses | SSE_n: 2.1995742321014404 | SSE_b: 0.33835917711257935 | Total: 2.537933349609375\n",
      "STEP: 260 | Avg Losses | SSE_n: 2.182509660720825 | SSE_b: 0.3362983465194702 | Total: 2.518807888031006\n",
      "STEP: 261 | Avg Losses | SSE_n: 2.175577402114868 | SSE_b: 0.33492136001586914 | Total: 2.5104987621307373\n",
      "STEP: 262 | Avg Losses | SSE_n: 2.158984899520874 | SSE_b: 0.33022913336753845 | Total: 2.4892139434814453\n",
      "STEP: 263 | Avg Losses | SSE_n: 2.195258617401123 | SSE_b: 0.3301320970058441 | Total: 2.525390625\n",
      "STEP: 264 | Avg Losses | SSE_n: 2.1550211906433105 | SSE_b: 0.3287457227706909 | Total: 2.483767032623291\n",
      "STEP: 265 | Avg Losses | SSE_n: 2.1480026245117188 | SSE_b: 0.3266031742095947 | Total: 2.4746057987213135\n",
      "STEP: 266 | Avg Losses | SSE_n: 2.1375083923339844 | SSE_b: 0.3226897716522217 | Total: 2.460198163986206\n",
      "STEP: 267 | Avg Losses | SSE_n: 2.126537322998047 | SSE_b: 0.31760019063949585 | Total: 2.4441375732421875\n",
      "STEP: 268 | Avg Losses | SSE_n: 2.110870361328125 | SSE_b: 0.3117598295211792 | Total: 2.4226303100585938\n",
      "STEP: 269 | Avg Losses | SSE_n: 2.084902048110962 | SSE_b: 0.30657529830932617 | Total: 2.391477346420288\n",
      "STEP: 270 | Avg Losses | SSE_n: 2.1375582218170166 | SSE_b: 0.3122507929801941 | Total: 2.4498090744018555\n",
      "STEP: 271 | Avg Losses | SSE_n: 2.0752832889556885 | SSE_b: 0.3009048402309418 | Total: 2.376188039779663\n",
      "STEP: 272 | Avg Losses | SSE_n: 2.0593814849853516 | SSE_b: 0.2991214096546173 | Total: 2.3585028648376465\n",
      "STEP: 273 | Avg Losses | SSE_n: 2.0512635707855225 | SSE_b: 0.2969735264778137 | Total: 2.3482370376586914\n",
      "STEP: 274 | Avg Losses | SSE_n: 2.0425970554351807 | SSE_b: 0.29480990767478943 | Total: 2.337406873703003\n",
      "STEP: 275 | Avg Losses | SSE_n: 2.028256416320801 | SSE_b: 0.2894335389137268 | Total: 2.317689895629883\n",
      "STEP: 276 | Avg Losses | SSE_n: 2.014345169067383 | SSE_b: 0.2913643419742584 | Total: 2.3057096004486084\n",
      "STEP: 277 | Avg Losses | SSE_n: 2.0066745281219482 | SSE_b: 0.286329448223114 | Total: 2.293004035949707\n",
      "STEP: 278 | Avg Losses | SSE_n: 1.9997806549072266 | SSE_b: 0.28296443819999695 | Total: 2.282745122909546\n",
      "STEP: 279 | Avg Losses | SSE_n: 1.9940290451049805 | SSE_b: 0.2811470329761505 | Total: 2.2751760482788086\n",
      "STEP: 280 | Avg Losses | SSE_n: 1.9738316535949707 | SSE_b: 0.2742674946784973 | Total: 2.2480990886688232\n",
      "STEP: 281 | Avg Losses | SSE_n: 1.9794681072235107 | SSE_b: 0.28140419721603394 | Total: 2.2608723640441895\n",
      "STEP: 282 | Avg Losses | SSE_n: 1.964586853981018 | SSE_b: 0.27274811267852783 | Total: 2.237334966659546\n",
      "STEP: 283 | Avg Losses | SSE_n: 1.9480326175689697 | SSE_b: 0.2685934901237488 | Total: 2.2166261672973633\n",
      "STEP: 284 | Avg Losses | SSE_n: 1.9328655004501343 | SSE_b: 0.2634925842285156 | Total: 2.1963582038879395\n",
      "STEP: 285 | Avg Losses | SSE_n: 1.902886152267456 | SSE_b: 0.25437870621681213 | Total: 2.1572649478912354\n",
      "STEP: 286 | Avg Losses | SSE_n: 1.8765559196472168 | SSE_b: 0.24990901350975037 | Total: 2.12646484375\n",
      "STEP: 287 | Avg Losses | SSE_n: 1.8498878479003906 | SSE_b: 0.24479790031909943 | Total: 2.0946857929229736\n",
      "STEP: 288 | Avg Losses | SSE_n: 1.8240025043487549 | SSE_b: 0.24241489171981812 | Total: 2.0664174556732178\n",
      "STEP: 289 | Avg Losses | SSE_n: 1.8067659139633179 | SSE_b: 0.23970568180084229 | Total: 2.04647159576416\n",
      "STEP: 290 | Avg Losses | SSE_n: 1.7866201400756836 | SSE_b: 0.23788031935691833 | Total: 2.0245003700256348\n",
      "STEP: 291 | Avg Losses | SSE_n: 1.7748349905014038 | SSE_b: 0.23690065741539001 | Total: 2.011735677719116\n",
      "STEP: 292 | Avg Losses | SSE_n: 1.758998990058899 | SSE_b: 0.23592975735664368 | Total: 1.9949287176132202\n",
      "STEP: 293 | Avg Losses | SSE_n: 1.7397621870040894 | SSE_b: 0.23671916127204895 | Total: 1.976481318473816\n",
      "STEP: 294 | Avg Losses | SSE_n: 1.7223258018493652 | SSE_b: 0.23860225081443787 | Total: 1.9609280824661255\n",
      "STEP: 295 | Avg Losses | SSE_n: 1.7126564979553223 | SSE_b: 0.23611721396446228 | Total: 1.948773741722107\n",
      "STEP: 296 | Avg Losses | SSE_n: 1.70442533493042 | SSE_b: 0.23166899383068085 | Total: 1.9360942840576172\n",
      "STEP: 297 | Avg Losses | SSE_n: 1.6997873783111572 | SSE_b: 0.23432905972003937 | Total: 1.9341164827346802\n",
      "STEP: 298 | Avg Losses | SSE_n: 1.6922900676727295 | SSE_b: 0.23084008693695068 | Total: 1.9231301546096802\n",
      "STEP: 299 | Avg Losses | SSE_n: 1.6834814548492432 | SSE_b: 0.23000012338161469 | Total: 1.913481593132019\n",
      "STEP: 300 | Avg Losses | SSE_n: 1.6720166206359863 | SSE_b: 0.23104095458984375 | Total: 1.90305757522583\n",
      "STEP: 301 | Avg Losses | SSE_n: 1.6589057445526123 | SSE_b: 0.23151817917823792 | Total: 1.8904238939285278\n",
      "STEP: 302 | Avg Losses | SSE_n: 1.6349458694458008 | SSE_b: 0.2293887734413147 | Total: 1.8643345832824707\n",
      "STEP: 303 | Avg Losses | SSE_n: 1.6137632131576538 | SSE_b: 0.2355775237083435 | Total: 1.8493406772613525\n",
      "STEP: 304 | Avg Losses | SSE_n: 1.6020901203155518 | SSE_b: 0.22900323569774628 | Total: 1.8310933113098145\n",
      "STEP: 305 | Avg Losses | SSE_n: 1.5958518981933594 | SSE_b: 0.22264458239078522 | Total: 1.8184964656829834\n",
      "STEP: 306 | Avg Losses | SSE_n: 1.5898231267929077 | SSE_b: 0.21881863474845886 | Total: 1.808641791343689\n",
      "STEP: 307 | Avg Losses | SSE_n: 1.5785659551620483 | SSE_b: 0.21640075743198395 | Total: 1.794966697692871\n",
      "STEP: 308 | Avg Losses | SSE_n: 1.5721079111099243 | SSE_b: 0.2211010903120041 | Total: 1.7932089567184448\n",
      "STEP: 309 | Avg Losses | SSE_n: 1.5686047077178955 | SSE_b: 0.21616381406784058 | Total: 1.7847685813903809\n",
      "STEP: 310 | Avg Losses | SSE_n: 1.5576508045196533 | SSE_b: 0.21840037405490875 | Total: 1.7760511636734009\n",
      "STEP: 311 | Avg Losses | SSE_n: 1.5490443706512451 | SSE_b: 0.2180512398481369 | Total: 1.7670955657958984\n",
      "STEP: 312 | Avg Losses | SSE_n: 1.5390888452529907 | SSE_b: 0.21809031069278717 | Total: 1.7571791410446167\n",
      "STEP: 313 | Avg Losses | SSE_n: 1.527137041091919 | SSE_b: 0.21725577116012573 | Total: 1.7443928718566895\n",
      "STEP: 314 | Avg Losses | SSE_n: 1.5023715496063232 | SSE_b: 0.2158803790807724 | Total: 1.7182519435882568\n",
      "STEP: 315 | Avg Losses | SSE_n: 1.4941623210906982 | SSE_b: 0.22033832967281342 | Total: 1.7145006656646729\n",
      "STEP: 316 | Avg Losses | SSE_n: 1.471426248550415 | SSE_b: 0.21743029356002808 | Total: 1.688856601715088\n",
      "STEP: 317 | Avg Losses | SSE_n: 1.465539574623108 | SSE_b: 0.21489907801151276 | Total: 1.6804386377334595\n",
      "STEP: 318 | Avg Losses | SSE_n: 1.4556020498275757 | SSE_b: 0.21455839276313782 | Total: 1.6701604127883911\n",
      "STEP: 319 | Avg Losses | SSE_n: 1.4704606533050537 | SSE_b: 0.21308596432209015 | Total: 1.6835466623306274\n",
      "STEP: 320 | Avg Losses | SSE_n: 1.4524998664855957 | SSE_b: 0.2134374976158142 | Total: 1.6659374237060547\n",
      "STEP: 321 | Avg Losses | SSE_n: 1.4430947303771973 | SSE_b: 0.21366477012634277 | Total: 1.65675950050354\n",
      "STEP: 322 | Avg Losses | SSE_n: 1.4292755126953125 | SSE_b: 0.21217574179172516 | Total: 1.6414512395858765\n",
      "STEP: 323 | Avg Losses | SSE_n: 1.4188501834869385 | SSE_b: 0.20982931554317474 | Total: 1.6286795139312744\n",
      "STEP: 324 | Avg Losses | SSE_n: 1.406216025352478 | SSE_b: 0.2060198038816452 | Total: 1.6122357845306396\n",
      "STEP: 325 | Avg Losses | SSE_n: 1.3812828063964844 | SSE_b: 0.2030854970216751 | Total: 1.584368348121643\n",
      "STEP: 326 | Avg Losses | SSE_n: 1.408865213394165 | SSE_b: 0.20065629482269287 | Total: 1.609521508216858\n",
      "STEP: 327 | Avg Losses | SSE_n: 1.3737633228302002 | SSE_b: 0.19712701439857483 | Total: 1.5708903074264526\n",
      "STEP: 328 | Avg Losses | SSE_n: 1.3598053455352783 | SSE_b: 0.19517749547958374 | Total: 1.5549829006195068\n",
      "STEP: 329 | Avg Losses | SSE_n: 1.3470072746276855 | SSE_b: 0.194035604596138 | Total: 1.5410429239273071\n",
      "STEP: 330 | Avg Losses | SSE_n: 1.3347294330596924 | SSE_b: 0.19091437757015228 | Total: 1.5256438255310059\n",
      "STEP: 331 | Avg Losses | SSE_n: 1.3109699487686157 | SSE_b: 0.1853860318660736 | Total: 1.4963560104370117\n",
      "STEP: 332 | Avg Losses | SSE_n: 1.4028326272964478 | SSE_b: 0.21618902683258057 | Total: 1.6190216541290283\n",
      "STEP: 333 | Avg Losses | SSE_n: 1.3060715198516846 | SSE_b: 0.1843981146812439 | Total: 1.4904696941375732\n",
      "STEP: 334 | Avg Losses | SSE_n: 1.2891604900360107 | SSE_b: 0.17971521615982056 | Total: 1.4688756465911865\n",
      "STEP: 335 | Avg Losses | SSE_n: 1.2784829139709473 | SSE_b: 0.17743277549743652 | Total: 1.4559156894683838\n",
      "STEP: 336 | Avg Losses | SSE_n: 1.2656843662261963 | SSE_b: 0.1731846034526825 | Total: 1.4388689994812012\n",
      "STEP: 337 | Avg Losses | SSE_n: 1.2516403198242188 | SSE_b: 0.17211279273033142 | Total: 1.4237531423568726\n",
      "STEP: 338 | Avg Losses | SSE_n: 1.2387709617614746 | SSE_b: 0.17348001897335052 | Total: 1.4122509956359863\n",
      "STEP: 339 | Avg Losses | SSE_n: 1.233034610748291 | SSE_b: 0.16732382774353027 | Total: 1.4003584384918213\n",
      "STEP: 340 | Avg Losses | SSE_n: 1.2282147407531738 | SSE_b: 0.16590826213359833 | Total: 1.3941229581832886\n",
      "STEP: 341 | Avg Losses | SSE_n: 1.2226476669311523 | SSE_b: 0.1646638959646225 | Total: 1.387311577796936\n",
      "STEP: 342 | Avg Losses | SSE_n: 1.2020838260650635 | SSE_b: 0.16443811357021332 | Total: 1.366521954536438\n",
      "STEP: 343 | Avg Losses | SSE_n: 1.1786048412322998 | SSE_b: 0.1628902703523636 | Total: 1.341495156288147\n",
      "STEP: 344 | Avg Losses | SSE_n: 1.159596562385559 | SSE_b: 0.1667880266904831 | Total: 1.3263845443725586\n",
      "STEP: 345 | Avg Losses | SSE_n: 1.1516125202178955 | SSE_b: 0.16237011551856995 | Total: 1.313982605934143\n",
      "STEP: 346 | Avg Losses | SSE_n: 1.1479947566986084 | SSE_b: 0.15908870100975037 | Total: 1.3070834875106812\n",
      "STEP: 347 | Avg Losses | SSE_n: 1.1425801515579224 | SSE_b: 0.15813866257667542 | Total: 1.3007187843322754\n",
      "STEP: 348 | Avg Losses | SSE_n: 1.1250591278076172 | SSE_b: 0.154851496219635 | Total: 1.2799105644226074\n",
      "STEP: 349 | Avg Losses | SSE_n: 1.1129292249679565 | SSE_b: 0.1541268229484558 | Total: 1.2670559883117676\n",
      "STEP: 350 | Avg Losses | SSE_n: 1.107304334640503 | SSE_b: 0.15126773715019226 | Total: 1.2585721015930176\n",
      "STEP: 351 | Avg Losses | SSE_n: 1.1014131307601929 | SSE_b: 0.15106014907360077 | Total: 1.25247323513031\n",
      "STEP: 352 | Avg Losses | SSE_n: 1.0908429622650146 | SSE_b: 0.14897866547107697 | Total: 1.2398216724395752\n",
      "STEP: 353 | Avg Losses | SSE_n: 1.0771005153656006 | SSE_b: 0.14952269196510315 | Total: 1.2266231775283813\n",
      "STEP: 354 | Avg Losses | SSE_n: 1.0703492164611816 | SSE_b: 0.1498170793056488 | Total: 1.2201663255691528\n",
      "STEP: 355 | Avg Losses | SSE_n: 1.0662777423858643 | SSE_b: 0.14642657339572906 | Total: 1.2127043008804321\n",
      "STEP: 356 | Avg Losses | SSE_n: 1.062134027481079 | SSE_b: 0.14555759727954865 | Total: 1.2076916694641113\n",
      "STEP: 357 | Avg Losses | SSE_n: 1.0539298057556152 | SSE_b: 0.14562015235424042 | Total: 1.199549913406372\n",
      "STEP: 358 | Avg Losses | SSE_n: 1.0402226448059082 | SSE_b: 0.14587640762329102 | Total: 1.1860990524291992\n",
      "STEP: 359 | Avg Losses | SSE_n: 1.019050121307373 | SSE_b: 0.14521421492099762 | Total: 1.1642643213272095\n",
      "STEP: 360 | Avg Losses | SSE_n: 1.0287764072418213 | SSE_b: 0.16263727843761444 | Total: 1.1914136409759521\n",
      "STEP: 361 | Avg Losses | SSE_n: 1.006481647491455 | SSE_b: 0.14733359217643738 | Total: 1.1538152694702148\n",
      "STEP: 362 | Avg Losses | SSE_n: 0.998096227645874 | SSE_b: 0.14655083417892456 | Total: 1.1446471214294434\n",
      "STEP: 363 | Avg Losses | SSE_n: 0.9918606877326965 | SSE_b: 0.1451076865196228 | Total: 1.1369683742523193\n",
      "STEP: 364 | Avg Losses | SSE_n: 0.9827157258987427 | SSE_b: 0.14447873830795288 | Total: 1.1271944046020508\n",
      "STEP: 365 | Avg Losses | SSE_n: 0.9673032760620117 | SSE_b: 0.14544221758842468 | Total: 1.1127455234527588\n",
      "STEP: 366 | Avg Losses | SSE_n: 0.954676628112793 | SSE_b: 0.15016792714595795 | Total: 1.104844570159912\n",
      "STEP: 367 | Avg Losses | SSE_n: 0.9502270221710205 | SSE_b: 0.14503711462020874 | Total: 1.095264196395874\n",
      "STEP: 368 | Avg Losses | SSE_n: 0.9472054839134216 | SSE_b: 0.1432965099811554 | Total: 1.0905020236968994\n",
      "STEP: 369 | Avg Losses | SSE_n: 0.9441303014755249 | SSE_b: 0.14302165806293488 | Total: 1.0871520042419434\n",
      "STEP: 370 | Avg Losses | SSE_n: 0.9357236623764038 | SSE_b: 0.14190229773521423 | Total: 1.0776259899139404\n",
      "STEP: 371 | Avg Losses | SSE_n: 0.9626106023788452 | SSE_b: 0.16033723950386047 | Total: 1.1229478120803833\n",
      "STEP: 372 | Avg Losses | SSE_n: 0.9324972629547119 | SSE_b: 0.14208485186100006 | Total: 1.0745820999145508\n",
      "STEP: 373 | Avg Losses | SSE_n: 0.9225714802742004 | SSE_b: 0.14158977568149567 | Total: 1.0641613006591797\n",
      "STEP: 374 | Avg Losses | SSE_n: 0.9159988760948181 | SSE_b: 0.1417434811592102 | Total: 1.0577423572540283\n",
      "STEP: 375 | Avg Losses | SSE_n: 0.909857451915741 | SSE_b: 0.14165809750556946 | Total: 1.0515155792236328\n",
      "STEP: 376 | Avg Losses | SSE_n: 0.9066539406776428 | SSE_b: 0.14117766916751862 | Total: 1.047831654548645\n",
      "STEP: 377 | Avg Losses | SSE_n: 0.9012141227722168 | SSE_b: 0.13968753814697266 | Total: 1.0409016609191895\n",
      "STEP: 378 | Avg Losses | SSE_n: 0.8982254266738892 | SSE_b: 0.13793854415416718 | Total: 1.0361639261245728\n",
      "STEP: 379 | Avg Losses | SSE_n: 0.8939412236213684 | SSE_b: 0.13688229024410248 | Total: 1.0308234691619873\n",
      "STEP: 380 | Avg Losses | SSE_n: 0.8864823579788208 | SSE_b: 0.1358519345521927 | Total: 1.022334337234497\n",
      "STEP: 381 | Avg Losses | SSE_n: 0.8672064542770386 | SSE_b: 0.1390315443277359 | Total: 1.0062379837036133\n",
      "STEP: 382 | Avg Losses | SSE_n: 0.8659505248069763 | SSE_b: 0.13526132702827454 | Total: 1.0012118816375732\n",
      "STEP: 383 | Avg Losses | SSE_n: 0.8558553457260132 | SSE_b: 0.13537923991680145 | Total: 0.9912346005439758\n",
      "STEP: 384 | Avg Losses | SSE_n: 0.8533176183700562 | SSE_b: 0.13496243953704834 | Total: 0.9882800579071045\n",
      "STEP: 385 | Avg Losses | SSE_n: 0.8480755090713501 | SSE_b: 0.13471952080726624 | Total: 0.982795000076294\n",
      "STEP: 386 | Avg Losses | SSE_n: 0.8376671075820923 | SSE_b: 0.13544389605522156 | Total: 0.9731110334396362\n",
      "STEP: 387 | Avg Losses | SSE_n: 0.8288366794586182 | SSE_b: 0.13518409430980682 | Total: 0.9640207886695862\n",
      "STEP: 388 | Avg Losses | SSE_n: 0.8271140456199646 | SSE_b: 0.13563385605812073 | Total: 0.9627479314804077\n",
      "STEP: 389 | Avg Losses | SSE_n: 0.8215463161468506 | SSE_b: 0.1340862512588501 | Total: 0.9556325674057007\n",
      "STEP: 390 | Avg Losses | SSE_n: 0.8199106454849243 | SSE_b: 0.1332651525735855 | Total: 0.9531757831573486\n",
      "STEP: 391 | Avg Losses | SSE_n: 0.8159706592559814 | SSE_b: 0.13187536597251892 | Total: 0.9478460550308228\n",
      "STEP: 392 | Avg Losses | SSE_n: 0.809973955154419 | SSE_b: 0.13128536939620972 | Total: 0.9412593245506287\n",
      "STEP: 393 | Avg Losses | SSE_n: 0.8243476748466492 | SSE_b: 0.14007654786109924 | Total: 0.9644242525100708\n",
      "STEP: 394 | Avg Losses | SSE_n: 0.8069630265235901 | SSE_b: 0.1308847963809967 | Total: 0.9378478527069092\n",
      "STEP: 395 | Avg Losses | SSE_n: 0.8011347055435181 | SSE_b: 0.13111485540866852 | Total: 0.9322495460510254\n",
      "STEP: 396 | Avg Losses | SSE_n: 0.7949631810188293 | SSE_b: 0.13221557438373566 | Total: 0.9271787405014038\n",
      "STEP: 397 | Avg Losses | SSE_n: 0.7893452644348145 | SSE_b: 0.1332072913646698 | Total: 0.9225525856018066\n",
      "STEP: 398 | Avg Losses | SSE_n: 0.7801344394683838 | SSE_b: 0.13402408361434937 | Total: 0.9141585230827332\n",
      "STEP: 399 | Avg Losses | SSE_n: 0.7646324038505554 | SSE_b: 0.13480275869369507 | Total: 0.8994351625442505\n",
      "STEP: 400 | Avg Losses | SSE_n: 0.785077691078186 | SSE_b: 0.14208677411079407 | Total: 0.9271644353866577\n",
      "STEP: 401 | Avg Losses | SSE_n: 0.7588061094284058 | SSE_b: 0.13333168625831604 | Total: 0.8921377658843994\n",
      "STEP: 402 | Avg Losses | SSE_n: 0.7440746426582336 | SSE_b: 0.1333637833595276 | Total: 0.8774384260177612\n",
      "STEP: 403 | Avg Losses | SSE_n: 0.7358839511871338 | SSE_b: 0.131813645362854 | Total: 0.8676975965499878\n",
      "STEP: 404 | Avg Losses | SSE_n: 0.7291696667671204 | SSE_b: 0.12925441563129425 | Total: 0.8584240674972534\n",
      "STEP: 405 | Avg Losses | SSE_n: 0.7226741313934326 | SSE_b: 0.12838108837604523 | Total: 0.8510552048683167\n",
      "STEP: 406 | Avg Losses | SSE_n: 0.7147815227508545 | SSE_b: 0.12824435532093048 | Total: 0.8430258631706238\n",
      "STEP: 407 | Avg Losses | SSE_n: 0.7099534273147583 | SSE_b: 0.12754181027412415 | Total: 0.8374952077865601\n",
      "STEP: 408 | Avg Losses | SSE_n: 0.7022852897644043 | SSE_b: 0.12903030216693878 | Total: 0.8313155770301819\n",
      "STEP: 409 | Avg Losses | SSE_n: 0.6993509531021118 | SSE_b: 0.12964437901973724 | Total: 0.8289953470230103\n",
      "STEP: 410 | Avg Losses | SSE_n: 0.6959018707275391 | SSE_b: 0.1274842917919159 | Total: 0.8233861923217773\n",
      "STEP: 411 | Avg Losses | SSE_n: 0.692497730255127 | SSE_b: 0.12493918836116791 | Total: 0.817436933517456\n",
      "STEP: 412 | Avg Losses | SSE_n: 0.6904232501983643 | SSE_b: 0.12352031469345093 | Total: 0.8139435648918152\n",
      "STEP: 413 | Avg Losses | SSE_n: 0.6870934367179871 | SSE_b: 0.12229529768228531 | Total: 0.8093887567520142\n",
      "STEP: 414 | Avg Losses | SSE_n: 0.6781883239746094 | SSE_b: 0.12094677239656448 | Total: 0.7991350889205933\n",
      "STEP: 415 | Avg Losses | SSE_n: 0.6646895408630371 | SSE_b: 0.11980804055929184 | Total: 0.7844975590705872\n",
      "STEP: 416 | Avg Losses | SSE_n: 0.6559890508651733 | SSE_b: 0.12130339443683624 | Total: 0.7772924304008484\n",
      "STEP: 417 | Avg Losses | SSE_n: 0.6502989530563354 | SSE_b: 0.11876722425222397 | Total: 0.7690661549568176\n",
      "STEP: 418 | Avg Losses | SSE_n: 0.6484782695770264 | SSE_b: 0.11828860640525818 | Total: 0.7667669057846069\n",
      "STEP: 419 | Avg Losses | SSE_n: 0.6427904963493347 | SSE_b: 0.11659561097621918 | Total: 0.7593861222267151\n",
      "STEP: 420 | Avg Losses | SSE_n: 0.6350388526916504 | SSE_b: 0.11893414705991745 | Total: 0.7539730072021484\n",
      "STEP: 421 | Avg Losses | SSE_n: 0.6294171810150146 | SSE_b: 0.11939924955368042 | Total: 0.7488164305686951\n",
      "STEP: 422 | Avg Losses | SSE_n: 0.6265190839767456 | SSE_b: 0.11558432877063751 | Total: 0.7421033978462219\n",
      "STEP: 423 | Avg Losses | SSE_n: 0.6252790689468384 | SSE_b: 0.11519457399845123 | Total: 0.7404736280441284\n",
      "STEP: 424 | Avg Losses | SSE_n: 0.6222679615020752 | SSE_b: 0.11445043236017227 | Total: 0.7367184162139893\n",
      "STEP: 425 | Avg Losses | SSE_n: 0.6185669898986816 | SSE_b: 0.11373073607683182 | Total: 0.7322977185249329\n",
      "STEP: 426 | Avg Losses | SSE_n: 0.6118220090866089 | SSE_b: 0.11299123615026474 | Total: 0.7248132228851318\n",
      "STEP: 427 | Avg Losses | SSE_n: 0.6176065802574158 | SSE_b: 0.11663596332073212 | Total: 0.7342425584793091\n",
      "STEP: 428 | Avg Losses | SSE_n: 0.6088371872901917 | SSE_b: 0.11202096939086914 | Total: 0.7208581566810608\n",
      "STEP: 429 | Avg Losses | SSE_n: 0.6060720682144165 | SSE_b: 0.11087848991155624 | Total: 0.716950535774231\n",
      "STEP: 430 | Avg Losses | SSE_n: 0.6039459705352783 | SSE_b: 0.10988765954971313 | Total: 0.7138336300849915\n",
      "STEP: 431 | Avg Losses | SSE_n: 0.6017431020736694 | SSE_b: 0.10935889184474945 | Total: 0.7111020088195801\n",
      "STEP: 432 | Avg Losses | SSE_n: 0.5956175327301025 | SSE_b: 0.10804691165685654 | Total: 0.7036644220352173\n",
      "STEP: 433 | Avg Losses | SSE_n: 0.6060134768486023 | SSE_b: 0.10861873626708984 | Total: 0.7146322131156921\n",
      "STEP: 434 | Avg Losses | SSE_n: 0.5937127470970154 | SSE_b: 0.10748668015003204 | Total: 0.7011994123458862\n",
      "STEP: 435 | Avg Losses | SSE_n: 0.5877289175987244 | SSE_b: 0.10714134573936462 | Total: 0.6948702335357666\n",
      "STEP: 436 | Avg Losses | SSE_n: 0.5841712355613708 | SSE_b: 0.10664470493793488 | Total: 0.6908159255981445\n",
      "STEP: 437 | Avg Losses | SSE_n: 0.5810854434967041 | SSE_b: 0.10601498186588287 | Total: 0.6871004104614258\n",
      "STEP: 438 | Avg Losses | SSE_n: 0.5776152610778809 | SSE_b: 0.10539703816175461 | Total: 0.6830123066902161\n",
      "STEP: 439 | Avg Losses | SSE_n: 0.5841463804244995 | SSE_b: 0.10672162473201752 | Total: 0.6908680200576782\n",
      "STEP: 440 | Avg Losses | SSE_n: 0.5758200287818909 | SSE_b: 0.10457196831703186 | Total: 0.6803920269012451\n",
      "STEP: 441 | Avg Losses | SSE_n: 0.5702375173568726 | SSE_b: 0.10415809601545334 | Total: 0.6743956208229065\n",
      "STEP: 442 | Avg Losses | SSE_n: 0.5667732954025269 | SSE_b: 0.10339765250682831 | Total: 0.6701709628105164\n",
      "STEP: 443 | Avg Losses | SSE_n: 0.5631782412528992 | SSE_b: 0.10270226001739502 | Total: 0.6658805012702942\n",
      "STEP: 444 | Avg Losses | SSE_n: 0.5592992305755615 | SSE_b: 0.10168570280075073 | Total: 0.6609849333763123\n",
      "STEP: 445 | Avg Losses | SSE_n: 0.5536215305328369 | SSE_b: 0.10123275220394135 | Total: 0.6548542976379395\n",
      "STEP: 446 | Avg Losses | SSE_n: 0.554749608039856 | SSE_b: 0.1020875871181488 | Total: 0.6568372249603271\n",
      "STEP: 447 | Avg Losses | SSE_n: 0.5512279272079468 | SSE_b: 0.10027759522199631 | Total: 0.6515055298805237\n",
      "STEP: 448 | Avg Losses | SSE_n: 0.5488581657409668 | SSE_b: 0.09954488277435303 | Total: 0.6484030485153198\n",
      "STEP: 449 | Avg Losses | SSE_n: 0.5449085235595703 | SSE_b: 0.09790798276662827 | Total: 0.6428164839744568\n",
      "STEP: 450 | Avg Losses | SSE_n: 0.5409137606620789 | SSE_b: 0.09616033732891083 | Total: 0.6370741128921509\n",
      "STEP: 451 | Avg Losses | SSE_n: 0.5386689901351929 | SSE_b: 0.09430098533630371 | Total: 0.6329699754714966\n",
      "STEP: 452 | Avg Losses | SSE_n: 0.5316176414489746 | SSE_b: 0.09328407049179077 | Total: 0.6249017119407654\n",
      "STEP: 453 | Avg Losses | SSE_n: 0.529729425907135 | SSE_b: 0.09248960763216019 | Total: 0.6222190260887146\n",
      "STEP: 454 | Avg Losses | SSE_n: 0.5250728726387024 | SSE_b: 0.09155549108982086 | Total: 0.6166283488273621\n",
      "STEP: 455 | Avg Losses | SSE_n: 0.5190956592559814 | SSE_b: 0.09099697321653366 | Total: 0.6100926399230957\n",
      "STEP: 456 | Avg Losses | SSE_n: 0.5150740742683411 | SSE_b: 0.08985284715890884 | Total: 0.6049269437789917\n",
      "STEP: 457 | Avg Losses | SSE_n: 0.5105260610580444 | SSE_b: 0.08928375691175461 | Total: 0.5998098254203796\n",
      "STEP: 458 | Avg Losses | SSE_n: 0.5071783065795898 | SSE_b: 0.08843296766281128 | Total: 0.5956112742424011\n",
      "STEP: 459 | Avg Losses | SSE_n: 0.5037220120429993 | SSE_b: 0.08772217482328415 | Total: 0.591444194316864\n",
      "STEP: 460 | Avg Losses | SSE_n: 0.50048828125 | SSE_b: 0.08759377896785736 | Total: 0.5880820751190186\n",
      "STEP: 461 | Avg Losses | SSE_n: 0.49996209144592285 | SSE_b: 0.0900082066655159 | Total: 0.5899702906608582\n",
      "STEP: 462 | Avg Losses | SSE_n: 0.49758315086364746 | SSE_b: 0.08739228546619415 | Total: 0.5849754214286804\n",
      "STEP: 463 | Avg Losses | SSE_n: 0.4908756613731384 | SSE_b: 0.08769458532333374 | Total: 0.5785702466964722\n",
      "STEP: 464 | Avg Losses | SSE_n: 0.4859697222709656 | SSE_b: 0.0877724140882492 | Total: 0.573742151260376\n",
      "STEP: 465 | Avg Losses | SSE_n: 0.4807448983192444 | SSE_b: 0.08804395794868469 | Total: 0.5687888860702515\n",
      "STEP: 466 | Avg Losses | SSE_n: 0.47742366790771484 | SSE_b: 0.08762023597955704 | Total: 0.5650439262390137\n",
      "STEP: 467 | Avg Losses | SSE_n: 0.49522799253463745 | SSE_b: 0.1066427081823349 | Total: 0.6018707156181335\n",
      "STEP: 468 | Avg Losses | SSE_n: 0.4756055772304535 | SSE_b: 0.08821778744459152 | Total: 0.5638233423233032\n",
      "STEP: 469 | Avg Losses | SSE_n: 0.47177982330322266 | SSE_b: 0.08700345456600189 | Total: 0.5587832927703857\n",
      "STEP: 470 | Avg Losses | SSE_n: 0.4695952236652374 | SSE_b: 0.08637222647666931 | Total: 0.5559674501419067\n",
      "STEP: 471 | Avg Losses | SSE_n: 0.46687448024749756 | SSE_b: 0.08595766872167587 | Total: 0.5528321266174316\n",
      "STEP: 472 | Avg Losses | SSE_n: 0.46383631229400635 | SSE_b: 0.08531837165355682 | Total: 0.5491546988487244\n",
      "STEP: 473 | Avg Losses | SSE_n: 0.4644101858139038 | SSE_b: 0.08594530075788498 | Total: 0.5503554940223694\n",
      "STEP: 474 | Avg Losses | SSE_n: 0.4614101052284241 | SSE_b: 0.08498787879943848 | Total: 0.5463979840278625\n",
      "STEP: 475 | Avg Losses | SSE_n: 0.45769211649894714 | SSE_b: 0.08410513401031494 | Total: 0.5417972803115845\n",
      "STEP: 476 | Avg Losses | SSE_n: 0.45638924837112427 | SSE_b: 0.08361140638589859 | Total: 0.5400006771087646\n",
      "STEP: 477 | Avg Losses | SSE_n: 0.45389416813850403 | SSE_b: 0.08254748582839966 | Total: 0.5364416837692261\n",
      "STEP: 478 | Avg Losses | SSE_n: 0.44982799887657166 | SSE_b: 0.08136831223964691 | Total: 0.5311962962150574\n",
      "STEP: 479 | Avg Losses | SSE_n: 0.4801672697067261 | SSE_b: 0.08555954694747925 | Total: 0.5657268166542053\n",
      "STEP: 480 | Avg Losses | SSE_n: 0.4495694637298584 | SSE_b: 0.0802091658115387 | Total: 0.5297785997390747\n",
      "STEP: 481 | Avg Losses | SSE_n: 0.44754236936569214 | SSE_b: 0.07880129665136337 | Total: 0.5263436436653137\n",
      "STEP: 482 | Avg Losses | SSE_n: 0.4455626606941223 | SSE_b: 0.07899019867181778 | Total: 0.5245528817176819\n",
      "STEP: 483 | Avg Losses | SSE_n: 0.44442102313041687 | SSE_b: 0.07850635051727295 | Total: 0.5229274034500122\n",
      "STEP: 484 | Avg Losses | SSE_n: 0.4418018162250519 | SSE_b: 0.07775092869997025 | Total: 0.5195527672767639\n",
      "STEP: 485 | Avg Losses | SSE_n: 0.43866342306137085 | SSE_b: 0.07696540653705597 | Total: 0.5156288146972656\n",
      "STEP: 486 | Avg Losses | SSE_n: 0.43478915095329285 | SSE_b: 0.0759548619389534 | Total: 0.510744035243988\n",
      "STEP: 487 | Avg Losses | SSE_n: 0.43460047245025635 | SSE_b: 0.07587951421737671 | Total: 0.5104799866676331\n",
      "STEP: 488 | Avg Losses | SSE_n: 0.4325348734855652 | SSE_b: 0.07513396441936493 | Total: 0.5076688528060913\n",
      "STEP: 489 | Avg Losses | SSE_n: 0.43096840381622314 | SSE_b: 0.07444363832473755 | Total: 0.5054120421409607\n",
      "STEP: 490 | Avg Losses | SSE_n: 0.4291650056838989 | SSE_b: 0.07412924617528915 | Total: 0.5032942295074463\n",
      "STEP: 491 | Avg Losses | SSE_n: 0.4272168278694153 | SSE_b: 0.07399855554103851 | Total: 0.501215398311615\n",
      "STEP: 492 | Avg Losses | SSE_n: 0.4207391142845154 | SSE_b: 0.07388205826282501 | Total: 0.4946211576461792\n",
      "STEP: 493 | Avg Losses | SSE_n: 0.4594874978065491 | SSE_b: 0.07799447327852249 | Total: 0.537481963634491\n",
      "STEP: 494 | Avg Losses | SSE_n: 0.41959553956985474 | SSE_b: 0.07384496927261353 | Total: 0.49344050884246826\n",
      "STEP: 495 | Avg Losses | SSE_n: 0.41502559185028076 | SSE_b: 0.07399822771549225 | Total: 0.4890238046646118\n",
      "STEP: 496 | Avg Losses | SSE_n: 0.41262081265449524 | SSE_b: 0.07391329854726791 | Total: 0.48653411865234375\n",
      "STEP: 497 | Avg Losses | SSE_n: 0.40937477350234985 | SSE_b: 0.07393190264701843 | Total: 0.4833066761493683\n",
      "STEP: 498 | Avg Losses | SSE_n: 0.40636348724365234 | SSE_b: 0.07403800636529922 | Total: 0.48040148615837097\n",
      "STEP: 499 | Avg Losses | SSE_n: 0.4037889838218689 | SSE_b: 0.07402538508176804 | Total: 0.47781437635421753\n",
      "STEP: 500 | Avg Losses | SSE_n: 0.4020334482192993 | SSE_b: 0.07356715947389603 | Total: 0.47560060024261475\n",
      "STEP: 501 | Avg Losses | SSE_n: 0.3993946313858032 | SSE_b: 0.07272227108478546 | Total: 0.4721168875694275\n",
      "STEP: 502 | Avg Losses | SSE_n: 0.39695316553115845 | SSE_b: 0.07220182567834854 | Total: 0.4691549837589264\n",
      "STEP: 503 | Avg Losses | SSE_n: 0.3921967148780823 | SSE_b: 0.07074444741010666 | Total: 0.46294116973876953\n",
      "STEP: 504 | Avg Losses | SSE_n: 0.3916940689086914 | SSE_b: 0.07271797955036163 | Total: 0.46441203355789185\n",
      "STEP: 505 | Avg Losses | SSE_n: 0.38894176483154297 | SSE_b: 0.07025671005249023 | Total: 0.4591984748840332\n",
      "STEP: 506 | Avg Losses | SSE_n: 0.385388046503067 | SSE_b: 0.06861918419599533 | Total: 0.45400723814964294\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/IRKPINN_Periodic.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep(closure)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[39m=\u001b[39m _strong_wolfe(\n\u001b[1;32m    427\u001b[0m         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[1;32m    428\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    429\u001b[0m opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/lbfgs.py:50\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     48\u001b[0m g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mclone(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcontiguous_format)\n\u001b[1;32m     49\u001b[0m \u001b[39m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m f_new, g_new \u001b[39m=\u001b[39m obj_func(x, t, d)\n\u001b[1;32m     51\u001b[0m ls_func_evals \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     52\u001b[0m gtd_new \u001b[39m=\u001b[39m g_new\u001b[39m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_directional_evaluate(closure, x, t, d)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_directional_evaluate\u001b[39m(\u001b[39mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 278\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[1;32m    279\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/IRKPINN_Periodic.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m outputs \u001b[39m=\u001b[39m pinn(\u001b[39minput\u001b[39m) \u001b[39m#run the batch through the current model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m overall_loss, SSE_n, SSE_b \u001b[39m=\u001b[39m SSE(\u001b[39minput\u001b[39m, outputs, initial_vals, coefficients, pinn, device) \u001b[39m#calculate the loss\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m overall_loss\u001b[39m.\u001b[39mbackward() \u001b[39m#Using backpropagation, calculate the gradients\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#print(f\"Avg loss: {loss.item()}\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN_Periodic.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mglobal\u001b[39;00m i\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def closure():\n",
    "    optimizer.zero_grad() #reset the gradient so that the previous iteration does not affect the current one\n",
    "    outputs = pinn(input) #run the batch through the current model\n",
    "    \n",
    "    overall_loss, SSE_n, SSE_b = SSE(input, outputs, initial_vals, coefficients, pinn, device) #calculate the loss\n",
    "    overall_loss.backward() #Using backpropagation, calculate the gradients\n",
    "    #print(f\"Avg loss: {loss.item()}\")\n",
    "\n",
    "    global i\n",
    "    #if i%100 == 0:\n",
    "    print(f\"STEP: {i} | Avg Losses | SSE_n: {SSE_n.item()} | SSE_b: {SSE_b.item()} | Total: {overall_loss.item()}\")\n",
    "    i += 1\n",
    "\n",
    "    return overall_loss\n",
    "\n",
    "optimizer.step(closure) #Using the gradients, adjust the parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_points = 1000\n",
    "\n",
    "# Generate x values\n",
    "x_vals = torch.linspace(-np.pi, np.pi, num_points).view(-1, 1).to(device)\n",
    "\n",
    "# Run the model to get predictions\n",
    "with torch.no_grad():\n",
    "    model_output = pinn(x_vals)\n",
    "\n",
    "# Take the q+1th output of the model (last output)\n",
    "final_output = model_output[:, -1].cpu().numpy()\n",
    "\n",
    "# Generate the original function values\n",
    "y_vals = 4*np.arctan(np.exp( -(x_vals.cpu().numpy()**2)/(2*(sigma**2)) ))\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals.cpu().numpy(), y_vals, 'r', label='-sin(pi * x)')\n",
    "plt.plot(x_vals.cpu().numpy(), final_output, 'b', label='Model Output')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Comparison of Model Output and -sin(pi * x)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "toSave = True\n",
    "\n",
    "if toSave:\n",
    "    base_path = \"./\"\n",
    "    base_name = \"irk_pinn_sine_gordon_periodic\"\n",
    "    extension = \".pth\"\n",
    "\n",
    "    counter = 0\n",
    "    model_save_path = os.path.join(base_path, base_name + extension)\n",
    "\n",
    "    while os.path.exists(model_save_path):\n",
    "        counter += 1\n",
    "        model_save_path = os.path.join(base_path, f\"{base_name}_{counter}{extension}\")\n",
    "\n",
    "    torch.save(pinn.state_dict(), model_save_path)\n",
    "\n",
    "    print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-1-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

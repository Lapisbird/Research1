{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "import pyDOE2\n",
    "from torch.autograd.functional import jacobian, hessian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bounds = [-1, 1]\n",
    "t_bounds = [0, 1]\n",
    "num_data_points = 100\n",
    "num_collocation_points = 10\n",
    "proportion_t_0 = 0.5 #the proportion of the data points which will exist at various points x along the boundary t = 0. The rest will be split between the boundaries x = -1 and x = 1 for all t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_t_0 = (int) (num_data_points * proportion_t_0)\n",
    "num_points_x_1 = (num_data_points - num_points_t_0)//2 # // is integer division\n",
    "num_points_x_neg_1 = num_data_points - num_points_t_0 - num_points_x_1\n",
    "\n",
    "#create num_data_points random data points on the boundaries of the PDE\n",
    "t_0_points = np.array( list( zip(np.zeros(num_points_t_0), 2 * np.random.rand(num_points_t_0) - 1 ) ) )\n",
    "x_1_points = np.array( list( zip(np.random.rand(num_points_x_1), np.full(num_points_x_1, 1)) ) ) #np.full() takes paramters shape, value. Shape can be a tuple for multidimensional arrays filled with value.\n",
    "x_neg_1_points = np.array( list( zip(np.random.rand(num_points_x_neg_1), np.full(num_points_x_neg_1, -1)) ) )\n",
    "x_points = np.concatenate(( x_1_points, x_neg_1_points ))\n",
    "\n",
    "#Generating labels with the data\n",
    "dtype = [('points', float, 2), ('label', float)] #need custom dtype because otherwise numpy doesn't like these combined arrays\n",
    "\n",
    "t_0_labels = -np.sin(np.pi * t_0_points[:,1] )\n",
    "t_0_combined = np.array(list( zip(t_0_points, t_0_labels) ), dtype=dtype)\n",
    "\n",
    "x_labels = np.zeros(num_points_x_1 + num_points_x_neg_1)\n",
    "x_combined = np.array(list( zip(x_points, x_labels) ), dtype=dtype)\n",
    "\n",
    "combined_labels_data = np.concatenate( (t_0_combined, x_combined) )\n",
    "\n",
    "np.random.shuffle(combined_labels_data)\n",
    "\n",
    "data_points, labels = map(np.array, map(list, zip(*combined_labels_data)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_DataSet(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "trainset = PINN_DataSet(data_points.astype(np.float32), labels.astype(np.float32)) #convert to float32, or else later the resulting torch tensors will be of torch.float64 type, which is not compatible with the neural network\n",
    "\n",
    "batch_size = num_data_points #no mini-batches\n",
    "\n",
    "num_workers = 0\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = num_workers\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocation Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhs_samples(n): #generate n collocation points via Latin Hypercube Sampling. Each point is a (t,x)\n",
    "    lhs_array = pyDOE2.lhs(2, samples=n) #Two dimensions. Values from 0 to 1\n",
    "    lhs_array[:,1] = 2*lhs_array[:,1] - 1 #convert range of x values to -1 to 1\n",
    "    return lhs_array\n",
    "\n",
    "collocation_points = lhs_samples(num_collocation_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential( #9 layers of 20 neurons each\n",
    "            nn.Linear(2,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,1),\n",
    "            nn.Tanh()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def MSE_f(collocation_points, neural_network, device):\n",
    "    collocation_inputs = torch.tensor(collocation_points.astype(np.float32), requires_grad=True).to(device) #make sure we send to correct device\n",
    "    u = neural_network(collocation_inputs)\n",
    "    u_t, u_x = torch.autograd.grad(u, collocation_inputs, grad_outputs=torch.ones_like(u), retain_graph=True) #torch.autograd.grad outputs a tuple with the same size as the number of input parameters. Each value in the tuple is a tensor in the same form as the network's output. retain_graph = True for future derivatives\n",
    "    u_xx = torch.autograd.grad(u_x, collocation_inputs[:,1]) #find second derivative of u_x with respect to x again\n",
    "    return torch.mean((u_t + u*u_xx - (0.01/torch.pi)*u_xx)**2)'''\n",
    "\n",
    "def MSE_f(collocation_points, neural_network, device):\n",
    "\n",
    "    #Note: I learned about lambda in python while doing this. Never got to use it, but just putting a note here because it seems quite cool\n",
    "\n",
    "    collocation_inputs = torch.tensor(collocation_points.astype(np.float32), requires_grad=True).to(device) #make sure we send to correct device\n",
    "\n",
    "    jacobian_matrix = jacobian(neural_network, collocation_inputs, create_graph=True) #Must use callable function as first parameter. Output of the form [batch_size, output_dim, input_dim]\n",
    "    u_t = torch.squeeze(jacobian_matrix[:,:,0]) #squeeze to get rid of useless dimensions of size 1\n",
    "    u_x = torch.squeeze(jacobian_matrix[:,:,1])\n",
    "    u_xx = torch.squeeze(hessian(neural_network, collocation_inputs, create_graph=True)[:,:,1]) #Not a huge fan of this whole implementation. The fact that I cannot create a second derivative off of the first one seems inefficient. And the fact that I am using the hessian, which necessiarily will compute a bunch of u_tt which I will never use. I think I could deal with that issue by defining the first jacobian as a method and then using that method as the first parameter in a second one. But I won't bother for now. More points of failure\n",
    "\n",
    "    return torch.mean((u_t + u*u_xx - (0.01/torch.pi)*u_xx)**2)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def criterion(output, label, collocation_points, neural_network, device): #collocation_points must be a NUMPY ARRAY\n",
    "    mse_u = nn.MSELoss()(output, label)\n",
    "    mse_f = MSE_f(collocation_points, neural_network, device)\n",
    "    return mse_u + mse_f, mse_u, mse_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexning/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The Tensor returned by the function given to hessian should contain a single element",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#reset the gradient so that the previous iteration does not affect the current one\u001b[39;00m\n\u001b[1;32m     21\u001b[0m output \u001b[39m=\u001b[39m pinn(\u001b[39minput\u001b[39m) \u001b[39m#run the batch through the current model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m loss, mse_u, mse_f \u001b[39m=\u001b[39m criterion(output, label, collocation_points, pinn, device) \u001b[39m#calculate the loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m#Using backpropagation, calculate the gradients\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m#Using the gradients, adjust the parameters (SGD with momentum in this case)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 26\u001b[0m, in \u001b[0;36mcriterion\u001b[0;34m(output, label, collocation_points, neural_network, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcriterion\u001b[39m(output, label, collocation_points, neural_network, device): \u001b[39m#collocation_points must be a NUMPY ARRAY\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     mse_u \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()(output, label)\n\u001b[0;32m---> 26\u001b[0m     mse_f \u001b[39m=\u001b[39m MSE_f(collocation_points, neural_network, device)\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m mse_u \u001b[39m+\u001b[39m mse_f, mse_u, mse_f\n",
      "Cell \u001b[0;32mIn[56], line 17\u001b[0m, in \u001b[0;36mMSE_f\u001b[0;34m(collocation_points, neural_network, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m u_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(jacobian_matrix[:,:,\u001b[39m0\u001b[39m]) \u001b[39m#squeeze to get rid of useless dimensions of size 1\u001b[39;00m\n\u001b[1;32m     16\u001b[0m u_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(jacobian_matrix[:,:,\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m u_xx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(hessian(neural_network, collocation_inputs, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:,:,\u001b[39m1\u001b[39m]) \u001b[39m#Not a huge fan of this whole implementation. The fact that I cannot create a second derivative off of the first one seems inefficient. And the fact that I am using the hessian, which necessiarily will compute a bunch of u_tt which I will never use. I think I could deal with that issue by defining the first jacobian as a method and then using that method as the first parameter in a second one. But I won't bother for now. More points of failure\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean((u_t \u001b[39m+\u001b[39m u\u001b[39m*\u001b[39mu_xx \u001b[39m-\u001b[39m (\u001b[39m0.01\u001b[39m\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39mpi)\u001b[39m*\u001b[39mu_xx)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/functional.py:826\u001b[0m, in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    823\u001b[0m     _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    824\u001b[0m     \u001b[39mreturn\u001b[39;00m jac\n\u001b[0;32m--> 826\u001b[0m res \u001b[39m=\u001b[39m jacobian(jac_func, inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, strict\u001b[39m=\u001b[39mstrict, vectorize\u001b[39m=\u001b[39mvectorize,\n\u001b[1;32m    827\u001b[0m                strategy\u001b[39m=\u001b[39mouter_jacobian_strategy)\n\u001b[1;32m    828\u001b[0m \u001b[39mreturn\u001b[39;00m _tuple_postprocess(res, (is_inputs_tuple, is_inputs_tuple))\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/functional.py:591\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    588\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    589\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 591\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39minputs)\n\u001b[1;32m    592\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    593\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    594\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/functional.py:822\u001b[0m, in \u001b[0;36mhessian.<locals>.jac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m outer_jacobian_strategy \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward-mode\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    819\u001b[0m     \u001b[39m# _grad_preprocess requires create_graph=True and input to require_grad\u001b[39;00m\n\u001b[1;32m    820\u001b[0m     \u001b[39m# or else the input will be detached\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     inp \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(t\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inp)\n\u001b[0;32m--> 822\u001b[0m jac \u001b[39m=\u001b[39m jacobian(ensure_single_output_function, inp, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    823\u001b[0m _check_requires_grad(jac, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n\u001b[1;32m    824\u001b[0m \u001b[39mreturn\u001b[39;00m jac\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/functional.py:591\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    588\u001b[0m is_inputs_tuple, inputs \u001b[39m=\u001b[39m _as_tuple(inputs, \u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    589\u001b[0m inputs \u001b[39m=\u001b[39m _grad_preprocess(inputs, create_graph\u001b[39m=\u001b[39mcreate_graph, need_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 591\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39minputs)\n\u001b[1;32m    592\u001b[0m is_outputs_tuple, outputs \u001b[39m=\u001b[39m _as_tuple(outputs,\n\u001b[1;32m    593\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39moutputs of the user-provided function\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    594\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mjacobian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m _check_requires_grad(outputs, \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m, strict\u001b[39m=\u001b[39mstrict)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/autograd/functional.py:813\u001b[0m, in \u001b[0;36mhessian.<locals>.ensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe function given to hessian should return a single Tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnelement() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe Tensor returned by the function given to hessian should contain a single element\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Tensor returned by the function given to hessian should contain a single element"
     ]
    }
   ],
   "source": [
    "pinn = PINN()\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters())\n",
    "\n",
    "num_epochs = 2 #I have no idea how many epochs were used in the paper's implementation. Let's just do a lot for now and see how quickly training converges\n",
    "\n",
    "#use the GPU to train if possible, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + (\"GPU\" if torch.cuda.is_available() else \"CPU\"))\n",
    "pinn.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_running_loss = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        input, label = data #input and label already seem to be tensors. Tbh, I am a little confused when this happened. I don't think I ever explicitly turned them into tensors. They used to both be numpy arrays. EDIT: Further research seems to show that dataloader does this automatically as it fetches data from the dataset\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad() #reset the gradient so that the previous iteration does not affect the current one\n",
    "        output = pinn(input) #run the batch through the current model\n",
    "        loss, mse_u, mse_f = criterion(output, label, collocation_points, pinn, device) #calculate the loss\n",
    "        loss.backward() #Using backpropagation, calculate the gradients\n",
    "        optimizer.step() #Using the gradients, adjust the parameters (SGD with momentum in this case)\n",
    "\n",
    "    printf(\"Avg MSE Loss Per Boundary Data Point: {mse_u}\")\n",
    "    printf(\"Avg f(t,x)^2 Per Collocation Point: {mse_f}\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-1-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

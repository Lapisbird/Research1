{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: I should defintely explore an object-oriented PyTorch approach in the future. It is annoying having to pass around every single required variable to helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of intermediate steps\n",
    "q = 500\n",
    "\n",
    "#step size\n",
    "h = 0.8\n",
    "\n",
    "#start and end location\n",
    "start_t = 0.1\n",
    "end_t = start_t + h\n",
    "\n",
    "N = None #number of x values for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREDIT TO https://github.com/maziarraissi/PINNs/blob/master/appendix/discrete_time_inference%20(Burgers)/Burgers_systematic.py FOR THE BELOW CODE TO IMPORT IRK WEIGHTS AND THE LABEL DATA (AND FOR THE DATA ITSELF)\n",
    "\n",
    "tmp = np.float32(np.loadtxt('./data/Butcher_IRK%d.txt' % (q), ndmin = 2))\n",
    "IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q)) #presume the first q rows and q columns represent a_{ij} values, and the final row represents the b_j values\n",
    "IRK_times = tmp[q**2+q:] #not sure what this is used for\n",
    "\n",
    "\n",
    "data = scipy.io.loadmat('./data/burgers_shock.mat')\n",
    "\n",
    "t_points = data['t'].flatten()[:,None] # T x 1\n",
    "x_points = data['x'].flatten()[:,None] # N x 1\n",
    "Exact = np.real(data['usol']).T # T x N\n",
    "\n",
    "#I may choose to generate my own data once more...\n",
    "'''#get data labels at t = 0.9 \n",
    "index = np.where(t_points == end_t)[0][0]\n",
    "end_labels = Exact[index, :]'''\n",
    "\n",
    "\n",
    "#MAKE SURE WHEN CREATE INPUTS that REQUIRES_GRADE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.act_func = nn.Tanh()\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, q + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nonelinear Burgers' Operator\n",
    "def NLO(input, outputs): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\n",
    "\n",
    "    u = outputs\n",
    "    u_x = torch.autograd.grad(u, input, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, input, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    return u * u_x - (0.01/torch.pi)*u_xx\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Implicit Runge-Kutta Calculation (Combine Intermediate Terms with Butcher Tableau Coefficients)\n",
    "def IRK(input, outputs, coefficients): #outputs is tensor of size Nx(q+1), represnting q stages for N x-values and the final n+1 prediction (which we ignore). coefficients is a tensor of size (q+1) x q representing the Butcher Tableau. We are not interested in its last row\n",
    "\n",
    "    outputs = outputs[:-1] #ignore last row of outputs - that is the n+1 prediction\n",
    "\n",
    "    global N\n",
    "    global q\n",
    "    global h\n",
    "\n",
    "    coefficients_a = coefficients[:-1] #all but last row (all the a_{ij} coefficients). Shape (q,q)\n",
    "    NLO_outputs = NLO(input, outputs) #perform non-linear operation on each of the outputs. Shape is still (N, q)\n",
    "    \n",
    "    N_coefficients_a = coefficients_a.repeat(N, 1, 1) #stack N coefficients_a tensors on top of each other\n",
    "    NLO_outputs = NLO_outputs.unsqueeze(-1) #reshape NLO_outputs_reshaped into shape (N, q, 1) for matrix multiplication\n",
    "\n",
    "    coefficients_applied = torch.bmm(N_coefficients_a, NLO_outputs) #perform matrix multiplication. This represents applying the coefficients a_{ij} from the Butcher Tableau to every element in NLO_outputs and THEN recombining them to make every new value which will go on to form the \"calculated\" outputs (intermediate stage values). Has shape (N, q, 1), since N_coefficients_a has shape (N, q, q) and NLO_outputs has shape (N, q, 1). For each layer in N, the matrix mulitplcaion (N, q) x (q, 1) takes place, resulting in a vector of shape (q, 1). N of these makes (N, q, 1)\n",
    "    coefficients_applied = coefficients_applied.view(N, -1) #now shape (N, q)\n",
    "\n",
    "    coefficients_applied = -h * coefficients_applied #apply the -h (-\\delta t) to each element in coefficients_applied\n",
    "    input = input.view(-1, 1) #reshape input from (N) to (N, 1)\n",
    "\n",
    "    result = coefficients_applied + input #Every element in coefficients_applied has a value added to it which is the element from input on the same layer. We can add the shapes (N, q) and (N, 1) due to \"broadcasting\", which effectively stretches the shape of the vector\n",
    "\n",
    "    return result #returns a result which is the same shape as \"outputs\" and represents the same intermediate values, except these are the \"calculated\" versions and not the \"direct predictions\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSE_n(input, outputs, coefficients, network, device):\n",
    "\n",
    "    coefficients = torch.tensor(coefficients).to(device)\n",
    "    calculated_outputs = IRK(input, outputs, coefficients, network)\n",
    "\n",
    "    SSE_n_loss = torch.sum((calculated_outputs - outputs)**2) + \n",
    "\n",
    "    return SSE_n_loss\n",
    "\n",
    "\n",
    "def SSE_b():\n",
    "\n",
    "\n",
    "def SEE():\n",
    "    return SSE_n + SSE_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

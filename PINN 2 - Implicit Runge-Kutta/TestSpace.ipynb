{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bunch of code, almost all from ChatGPT, attempting to find functional approaches of obtaining a batch hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Jacobian: [[[1.219303]\n",
      "  [0.34945 ]]\n",
      "\n",
      " [[1.219303]\n",
      "  [0.34945 ]]\n",
      "\n",
      " [[1.219303]\n",
      "  [0.34945 ]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple neural network model with 1 input and 2 outputs\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(1,), activation='linear')  # No activation to keep it simple\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = tf.constant([[1.0], [2.0], [3.0]])  # Shape (3, 1) - A batch of 3 samples\n",
    "\n",
    "# Use GradientTape to monitor the input for gradient computation\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(input_data)\n",
    "    \n",
    "    # Forward pass\n",
    "    output_data = model(input_data)\n",
    "\n",
    "# Compute the batch Jacobian\n",
    "batch_jacobian = tape.batch_jacobian(output_data, input_data)\n",
    "\n",
    "# Clean up resources of GradientTape\n",
    "del tape\n",
    "\n",
    "# Print the batch Jacobian\n",
    "print(\"Batch Jacobian:\", batch_jacobian.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Jacobian: [[[-0.9224357]\n",
      "  [-1.0878837]]\n",
      "\n",
      " [[-0.9224357]\n",
      "  [-1.0878837]]\n",
      "\n",
      " [[-0.9224357]\n",
      "  [-1.0878837]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple neural network model with 1 input and 2 outputs\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(1,), activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = tf.constant([[1.0], [2.0], [3.0]])  # Shape (3, 1) - A batch of 3 samples\n",
    "\n",
    "def compute_batch_jacobian(tape, input_data, output_data):\n",
    "    # Compute the batch Jacobian using the provided GradientTape\n",
    "    batch_jacobian = tape.batch_jacobian(output_data, input_data)\n",
    "    \n",
    "    return batch_jacobian\n",
    "\n",
    "# Forward pass to get the output data\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(input_data)\n",
    "    output_data = model(input_data)\n",
    "\n",
    "# Call the function to compute the batch Jacobian\n",
    "batch_jacobian = compute_batch_jacobian(tape, input_data, output_data)\n",
    "\n",
    "# Clean up resources of GradientTape\n",
    "del tape\n",
    "\n",
    "# Print the batch Jacobian\n",
    "print(\"Batch Jacobian:\", batch_jacobian.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Jacobian: [[[-0.9105931 ]\n",
      "  [ 0.13474047]]\n",
      "\n",
      " [[-0.9105931 ]\n",
      "  [ 0.13474047]]\n",
      "\n",
      " [[-0.9105931 ]\n",
      "  [ 0.13474047]]]\n",
      "Second Derivatives: [[[[[-1.5517052]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-1.5517052]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-1.5517052]]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a simple neural network model with 1 input and 2 outputs\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(1,), activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)  # Shape (3, 1) - A batch of 3 samples\n",
    "\n",
    "# First and Second Derivative calculation\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    outer_tape.watch(input_data)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        inner_tape.watch(input_data)\n",
    "        \n",
    "        # Forward pass\n",
    "        output_data = model(input_data)\n",
    "\n",
    "    # First derivative (Jacobian)\n",
    "    jacobian = outer_tape.batch_jacobian(output_data, input_data)\n",
    "    \n",
    "    # Compute gradients (first derivatives) with respect to input\n",
    "    gradients = inner_tape.gradient(output_data, input_data)  # shape will be (3, 1)\n",
    "\n",
    "# Check if gradients are None\n",
    "if gradients is not None:\n",
    "    gradients = tf.reshape(gradients, [-1, 1, 1])  # reshape to (3, 1, 1) to match batch jacobian shape\n",
    "    \n",
    "    # Second derivative\n",
    "    with tf.GradientTape(persistent=True) as second_tape:\n",
    "        second_tape.watch(gradients)\n",
    "        \n",
    "        # \"Fake\" forward pass to enable second derivative computation\n",
    "        dummy_output = gradients * gradients  # just square it, shape will be (3, 1, 1)\n",
    "        \n",
    "    # Compute second derivatives\n",
    "    second_derivatives = second_tape.batch_jacobian(dummy_output, gradients)  # should be of shape (3, 1, 1, 1)\n",
    "    \n",
    "    del second_tape\n",
    "else:\n",
    "    print(\"First derivatives could not be computed.\")\n",
    "    second_derivatives = None\n",
    "\n",
    "# Clean up resources of GradientTape\n",
    "del outer_tape\n",
    "del inner_tape\n",
    "\n",
    "# Print the batch Jacobian and second derivatives\n",
    "print(\"Batch Jacobian:\", jacobian.numpy())\n",
    "if second_derivatives is not None:\n",
    "    print(\"Second Derivatives:\", second_derivatives.numpy())\n",
    "else:\n",
    "    print(\"Second Derivatives could not be computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Data: [[-1.0177648  0.7162975]\n",
      " [-2.0355296  1.432595 ]\n",
      " [-3.0532944  2.1488924]]\n",
      "First-order gradients (Jacobian): [[-0.3014673]\n",
      " [-0.3014673]\n",
      " [-0.3014673]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/TestSpace.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput Data:\u001b[39m\u001b[39m\"\u001b[39m, output_data\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst-order gradients (Jacobian):\u001b[39m\u001b[39m\"\u001b[39m, grads1\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSecond-order gradients:\u001b[39m\u001b[39m\"\u001b[39m, grads2\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def compute_gradients(input_data, model):\n",
    "    with tf.GradientTape(persistent=True) as tape1:\n",
    "        tape1.watch(input_data)\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(input_data)\n",
    "            output_data = model(input_data)\n",
    "        \n",
    "        # First-order gradients (Jacobian)\n",
    "        grads1 = tape2.gradient(output_data, input_data)\n",
    "    \n",
    "    # Second-order gradients\n",
    "    grads2 = tape1.gradient(grads1, input_data)\n",
    "    \n",
    "    del tape1\n",
    "    del tape2\n",
    "    \n",
    "    return output_data, grads1, grads2\n",
    "\n",
    "# Create a simple neural network model with 1 input and 2 outputs\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(1,), activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Generate some dummy data\n",
    "input_data = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)  # Shape (3, 1) - A batch of 3 samples\n",
    "\n",
    "# Compute gradients\n",
    "output_data, grads1, grads2 = compute_gradients(input_data, model)\n",
    "\n",
    "print(\"Output Data:\", output_data.numpy())\n",
    "print(\"First-order gradients (Jacobian):\", grads1.numpy())\n",
    "print(\"Second-order gradients:\", grads2.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 10:47:30.450842: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.460909: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.469517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.474715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.574362: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.584525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.592618: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n",
      "2023-10-06 10:47:30.597064: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/gradient_tape/mul_1/pfor/Mul_grad/Reshape/pfor/concat/loop_body/PartitionedCall/pfor/Reshape}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian for y1: tf.Tensor(\n",
      "[[[2. 0. 0.]\n",
      "  [0. 4. 0.]\n",
      "  [0. 0. 6.]]], shape=(1, 3, 3), dtype=float32)\n",
      "Jacobian for y2: tf.Tensor(\n",
      "[[[ 3.  0.  0.]\n",
      "  [ 0. 12.  0.]\n",
      "  [ 0.  0. 27.]]], shape=(1, 3, 3), dtype=float32)\n",
      "Hessian for y1: tf.Tensor(\n",
      "[[[[2. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 2. 0.]\n",
      "   [0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0.]\n",
      "   [0. 0. 0.]\n",
      "   [0. 0. 2.]]]], shape=(1, 3, 3, 3), dtype=float32)\n",
      "Hessian for y2: tf.Tensor(\n",
      "[[[[ 6.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0. 12.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0. 18.]]]], shape=(1, 3, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple function with two outputs and one input\n",
    "def my_function(x):\n",
    "    return x ** 2, x ** 3\n",
    "\n",
    "# Create an input tensor\n",
    "x = tf.constant([[1.0, 2.0, 3.0]], dtype=tf.float32)\n",
    "\n",
    "# Initialize outer GradientTape\n",
    "with tf.GradientTape(persistent=True) as outer_tape:\n",
    "    outer_tape.watch(x)\n",
    "    \n",
    "    # Initialize inner GradientTape\n",
    "    with tf.GradientTape(persistent=True) as inner_tape:\n",
    "        inner_tape.watch(x)\n",
    "        \n",
    "        # Compute function values\n",
    "        y1, y2 = my_function(x)\n",
    "    \n",
    "    # Compute first derivatives (Jacobians)\n",
    "    jacobian_y1 = inner_tape.batch_jacobian(y1, x)\n",
    "    jacobian_y2 = inner_tape.batch_jacobian(y2, x)\n",
    "    \n",
    "# Compute second derivatives (Hessians)\n",
    "hessian_y1 = outer_tape.batch_jacobian(jacobian_y1, x)\n",
    "hessian_y2 = outer_tape.batch_jacobian(jacobian_y2, x)\n",
    "\n",
    "# Delete tapes to free resources\n",
    "del inner_tape\n",
    "del outer_tape\n",
    "\n",
    "print(\"Jacobian for y1:\", jacobian_y1)\n",
    "print(\"Jacobian for y2:\", jacobian_y2)\n",
    "print(\"Hessian for y1:\", hessian_y1)\n",
    "print(\"Hessian for y2:\", hessian_y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/TestSpace.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         y_i \u001b[39m=\u001b[39m y[:, i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     gradient_i \u001b[39m=\u001b[39m inner_tape\u001b[39m.\u001b[39mgradient(y_i, x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     hessian_i \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(gradient_i, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     second_derivatives \u001b[39m=\u001b[39m second_derivatives\u001b[39m.\u001b[39mwrite(i, hessian_i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/TestSpace.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Stack the second derivatives to form a [N, q] matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-2-ai-tf/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1021\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     logging\u001b[39m.\u001b[39mlog_first_n(\n\u001b[1;32m   1010\u001b[0m         logging\u001b[39m.\u001b[39mWARN, \u001b[39m\"\u001b[39m\u001b[39mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtape inside its context is significantly less \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgradient in order to compute higher order \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mderivatives.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mArgument `target` should be a list or nested structure\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mdifferentiated, but received None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1025\u001b[0m flat_targets \u001b[39m=\u001b[39m []\n\u001b[1;32m   1026\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(target):\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple function with q=2 outputs and one input\n",
    "# f(x) = [x^2, x^3]\n",
    "def my_function(x):\n",
    "    return tf.stack([x ** 2, x ** 3], axis=-1)\n",
    "\n",
    "# Create an input tensor (batch size N=3)\n",
    "x = tf.constant([[1.0, 2.0, 3.0]], dtype=tf.float32)\n",
    "\n",
    "# Initialize the GradientTape\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = my_function(x)  # Shape: [N, q]\n",
    "\n",
    "# Compute the Jacobian (first derivatives)\n",
    "jacobian = tape.batch_jacobian(y, x)  # Shape: [N, q]\n",
    "\n",
    "# Initialize a tensor to store the second derivatives\n",
    "second_derivatives = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "# Loop to compute the second derivatives for each output\n",
    "for i in range(y.shape[-1]):\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        inner_tape.watch(x)\n",
    "        y_i = y[:, i]\n",
    "    gradient_i = inner_tape.gradient(y_i, x)\n",
    "    hessian_i = tape.gradient(gradient_i, x)\n",
    "    second_derivatives = second_derivatives.write(i, hessian_i)\n",
    "\n",
    "# Stack the second derivatives to form a [N, q] matrix\n",
    "second_derivatives_matrix = tf.transpose(second_derivatives.stack())\n",
    "\n",
    "# Delete the tape to free resources\n",
    "del tape\n",
    "\n",
    "jacobian, second_derivatives_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-2-ai-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

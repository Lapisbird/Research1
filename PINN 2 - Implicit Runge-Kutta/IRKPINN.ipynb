{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: I should defintely explore an object-oriented PyTorch approach in the future. It is annoying having to pass around every single required variable to helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io\n",
    "\n",
    "from torch.autograd.functional import jacobian\n",
    "from torch.autograd.functional import hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of intermediate steps\n",
    "q = 500\n",
    "\n",
    "#step size\n",
    "h = 0.8\n",
    "\n",
    "#start and end location\n",
    "start_t = 0.1\n",
    "end_t = start_t + h\n",
    "\n",
    "N = 100 #number of x values for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata = scipy.io.loadmat('./data/burgers_shock.mat')\\n\\nt_points = data['t'].flatten()[:,None] # T x 1\\nx_points = data['x'].flatten()[:,None] # N x 1\\nExact = np.real(data['usol']).T # T x N\\n\\n#get data labels at t = 0.9 \\nindex = np.where(t_points == end_t)[0][0]\\nend_labels = Exact[index, :]\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREDIT TO https://github.com/maziarraissi/PINNs/blob/master/appendix/discrete_time_inference%20(Burgers)/Burgers_systematic.py FOR THE BELOW CODE TO IMPORT IRK WEIGHTS AND THE LABEL DATA (AND FOR THE DATA ITSELF)\n",
    "\n",
    "tmp = np.float32(np.loadtxt('./data/Butcher_IRK%d.txt' % (q), ndmin = 2))\n",
    "IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q)) #presume the first q rows and q columns represent a_{ij} values, and the final row represents the b_j values\n",
    "IRK_times = tmp[q**2+q:] #not sure what this is used for\n",
    "\n",
    "\n",
    "#I may choose to generate my own data once more...\n",
    "'''\n",
    "data = scipy.io.loadmat('./data/burgers_shock.mat')\n",
    "\n",
    "t_points = data['t'].flatten()[:,None] # T x 1\n",
    "x_points = data['x'].flatten()[:,None] # N x 1\n",
    "Exact = np.real(data['usol']).T # T x N\n",
    "\n",
    "#get data labels at t = 0.9 \n",
    "index = np.where(t_points == end_t)[0][0]\n",
    "end_labels = Exact[index, :]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(N) * 2 - 1 #x values between -1 and 1\n",
    "\n",
    "initial_vals = -np.sin(input * np.pi) #initial values at t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.act_func = nn.Tanh()\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, 50),\n",
    "            self.act_func,\n",
    "            nn.Linear(50, q + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Nonelinear Burgers' Operator\\ndef NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\\n\\n\\n    u = outputs\\n    print(input.shape)\\n    print(u.shape)\\n\\n    u_x = jacobian(lambda u,x: u, (u, input), create_graph=True, vectorize=True)\\n\\n    print(len(u_x))\\n    print(u_x[0].shape)\\n    print(u_x[1].shape)\\n\\n    u_x = u_x.squeeze(-1) #Jacobian with vectorize should return shape (N, q, 1)\\n\\n    u_xx = jacobian(lambda u_x, x: u_x, (u_x, input), create_graph=True, vectorize=True)\\n\\n    u_xx = u_xx.squeeze(-1)\\n\\n\\n    return u * u_x - (0.01/torch.pi)*u_xx\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Nonelinear Burgers' Operator\n",
    "def NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\n",
    "\n",
    "\n",
    "    u = outputs\n",
    "    print(input.shape)\n",
    "    print(u.shape)\n",
    "\n",
    "    u_x = jacobian(lambda u,x: u, (u, input), create_graph=True, vectorize=True)\n",
    "\n",
    "    print(len(u_x))\n",
    "    print(u_x[0].shape)\n",
    "    print(u_x[1].shape)\n",
    "\n",
    "    u_x = u_x.squeeze(-1) #Jacobian with vectorize should return shape (N, q, 1)\n",
    "\n",
    "    u_xx = jacobian(lambda u_x, x: u_x, (u_x, input), create_graph=True, vectorize=True)\n",
    "\n",
    "    u_xx = u_xx.squeeze(-1)\n",
    "\n",
    "\n",
    "    return u * u_x - (0.01/torch.pi)*u_xx'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#Nonelinear Burgers\\' Operator\\ndef NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\\n\\n\\n    global N\\n    global q\\n\\n    final_output = torch.empty(N, q, requires_grad=True).to(device).to(torch.float32)\\n\\n    u = outputs.to(torch.float32)\\n\\n    for i in range(q):\\n\\n        new_u = u[:, i].view(-1, 1).to(torch.float32)\\n\\n        u_x = torch.autograd.grad(new_u, input, grad_outputs=torch.ones_like(new_u), retain_graph=True, create_graph=True)[0].to(torch.float32)\\n        u_xx = torch.autograd.grad(u_x, input, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0].to(torch.float32)\\n\\n        #print(new_u.shape)\\n        #print(u_x.shape)\\n        #print(u_xx.shape)\\n        #print(final_output[:, i].shape)\\n        new_column = (new_u * u_x - (0.01/torch.pi)*u_xx).squeeze(-1).to(device).to(torch.float32)\\n        final_output = torch.cat([final_output[:, :i], new_column.unsqueeze(1), final_output[:, i+1:]], dim=1).to(device).to(torch.float32)\\n\\n\\n\\n    return final_output\\n\\n\\n\\n    #return outputs\\n\\n    \\n    \\n\\n\\n#Implicit Runge-Kutta Calculation (Combine Intermediate Terms with Butcher Tableau Coefficients)\\ndef IRK(input, outputs, initial_vals, coefficients, device): #outputs is tensor of size Nx(q+1), represnting q stages for N x-values and the final n+1 prediction. coefficients is a tensor of size (q+1) x q representing the Butcher Tableau. Input and initial_vals are size (N)\\n\\n    outputs_truncated = outputs[:, :-1] #get all but last row\\n\\n    global N\\n    global q\\n    global h\\n\\n\\n    NLO_outputs = NLO(input, outputs_truncated, device) #perform non-linear operation on each of the outputs_truncated. Shape is still (N, q)\\n    \\n    N_coefficients = coefficients.repeat(N, 1, 1) #stack N coefficients tensors on top of each other\\n    NLO_outputs = NLO_outputs.unsqueeze(-1) #reshape NLO_outputs_reshaped into shape (N, q, 1) for matrix multiplication\\n\\n    coefficients_applied = torch.bmm(N_coefficients, NLO_outputs) #perform matrix multiplication. This represents applying the coefficients a_{ij} (and b_i for n+1 prediction) from the Butcher Tableau to every element in NLO_outputs and THEN recombining them to make every new value which will go on to form the \"calculated\" outputs (intermediate stage values). Has shape (N, q+1, 1), since N_coefficients has shape (N, q+1, q) and NLO_outputs has shape (N, q, 1). For each layer in N, the matrix mulitplcaion (q+1, q) x (q, 1) takes place, resulting in a vector of shape (q+1, 1). N of these makes (N, q+1, 1)\\n    coefficients_applied = coefficients_applied.view(N, -1) #now shape (N, q + 1)\\n\\n    coefficients_applied = -h * coefficients_applied #apply the -h (-\\\\delta t) to each element in coefficients_applied\\n    initial_vals = initial_vals.view(-1, 1) #reshape initial_vals from (N) to (N, 1)\\n\\n    result = coefficients_applied + initial_vals #Every element in coefficients_applied has a value added to it which is the element from initial_vals on the same layer. We can add the shapes (N, q+1) and (N, 1) due to \"broadcasting\", which effectively stretches the shape of the vector\\n\\n    return result #returns a result which is the same shape as \"outputs\" and represents the same intermediate values, except these are the \"calculated\" versions and not the \"direct predictions\"\\n\\n\\n    \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "#Nonelinear Burgers' Operator\n",
    "def NLO(input, outputs, device): #outputs is of the form N x q. Inputs is of form N x 1 and was used to get outputs\n",
    "\n",
    "\n",
    "    global N\n",
    "    global q\n",
    "\n",
    "    final_output = torch.empty(N, q, requires_grad=True).to(device).to(torch.float32)\n",
    "\n",
    "    u = outputs.to(torch.float32)\n",
    "\n",
    "    for i in range(q):\n",
    "\n",
    "        new_u = u[:, i].view(-1, 1).to(torch.float32)\n",
    "\n",
    "        u_x = torch.autograd.grad(new_u, input, grad_outputs=torch.ones_like(new_u), retain_graph=True, create_graph=True)[0].to(torch.float32)\n",
    "        u_xx = torch.autograd.grad(u_x, input, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0].to(torch.float32)\n",
    "\n",
    "        #print(new_u.shape)\n",
    "        #print(u_x.shape)\n",
    "        #print(u_xx.shape)\n",
    "        #print(final_output[:, i].shape)\n",
    "        new_column = (new_u * u_x - (0.01/torch.pi)*u_xx).squeeze(-1).to(device).to(torch.float32)\n",
    "        final_output = torch.cat([final_output[:, :i], new_column.unsqueeze(1), final_output[:, i+1:]], dim=1).to(device).to(torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "\n",
    "    #return outputs\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#Implicit Runge-Kutta Calculation (Combine Intermediate Terms with Butcher Tableau Coefficients)\n",
    "def IRK(input, outputs, initial_vals, coefficients, device): #outputs is tensor of size Nx(q+1), represnting q stages for N x-values and the final n+1 prediction. coefficients is a tensor of size (q+1) x q representing the Butcher Tableau. Input and initial_vals are size (N)\n",
    "\n",
    "    outputs_truncated = outputs[:, :-1] #get all but last row\n",
    "\n",
    "    global N\n",
    "    global q\n",
    "    global h\n",
    "\n",
    "\n",
    "    NLO_outputs = NLO(input, outputs_truncated, device) #perform non-linear operation on each of the outputs_truncated. Shape is still (N, q)\n",
    "    \n",
    "    N_coefficients = coefficients.repeat(N, 1, 1) #stack N coefficients tensors on top of each other\n",
    "    NLO_outputs = NLO_outputs.unsqueeze(-1) #reshape NLO_outputs_reshaped into shape (N, q, 1) for matrix multiplication\n",
    "\n",
    "    coefficients_applied = torch.bmm(N_coefficients, NLO_outputs) #perform matrix multiplication. This represents applying the coefficients a_{ij} (and b_i for n+1 prediction) from the Butcher Tableau to every element in NLO_outputs and THEN recombining them to make every new value which will go on to form the \"calculated\" outputs (intermediate stage values). Has shape (N, q+1, 1), since N_coefficients has shape (N, q+1, q) and NLO_outputs has shape (N, q, 1). For each layer in N, the matrix mulitplcaion (q+1, q) x (q, 1) takes place, resulting in a vector of shape (q+1, 1). N of these makes (N, q+1, 1)\n",
    "    coefficients_applied = coefficients_applied.view(N, -1) #now shape (N, q + 1)\n",
    "\n",
    "    coefficients_applied = -h * coefficients_applied #apply the -h (-\\delta t) to each element in coefficients_applied\n",
    "    initial_vals = initial_vals.view(-1, 1) #reshape initial_vals from (N) to (N, 1)\n",
    "\n",
    "    result = coefficients_applied + initial_vals #Every element in coefficients_applied has a value added to it which is the element from initial_vals on the same layer. We can add the shapes (N, q+1) and (N, 1) due to \"broadcasting\", which effectively stretches the shape of the vector\n",
    "\n",
    "    return result #returns a result which is the same shape as \"outputs\" and represents the same intermediate values, except these are the \"calculated\" versions and not the \"direct predictions\"\n",
    "\n",
    "\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "def SSE_n(input, outputs, initial_vals, coefficients, device):\n",
    "\n",
    "    calculated_outputs = IRK(input, outputs, initial_vals, coefficients, device)\n",
    "\n",
    "    SSE_n_loss = torch.sum((calculated_outputs - outputs)**2)\n",
    "\n",
    "    return SSE_n_loss\n",
    "\n",
    "\n",
    "def SSE_b(network, device):\n",
    "    boundary_points = torch.tensor([-1, 1]).to(device).view(-1, 1)\n",
    "    boundary_results = network(boundary_points) #get all intermediary points and the n+1 prediction for x = -1 and 1\n",
    "    return torch.sum(boundary_results**2) #square results and sum them\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def SSE(input, outputs, initial_vals, coefficients, network, device):\n",
    "    #SSE_n_loss = SSE_n(input, outputs, initial_vals, coefficients, device)\n",
    "    #SSE_b_loss = SSE_b(network, device)\n",
    "    return torch.sum(outputs) #SSE_n_loss + SSE_b_loss, SSE_n, SSE_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "pinn = PINN()\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters(), #PARAMETERS CREDIT TO https://github.com/teeratornk/PINNs-2/blob/master/Burgers%20Equation/Burgers%20Inference%20(PyTorch).ipynb\n",
    "                              lr=1,\n",
    "                              max_iter=50000, \n",
    "                                max_eval=50000, \n",
    "                                history_size=50,\n",
    "                                tolerance_grad=1e-5, \n",
    "                                tolerance_change=1.0 * np.finfo(float).eps,\n",
    "                                line_search_fn=\"strong_wolfe\"\n",
    "                              )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + (\"GPU\" if torch.cuda.is_available() else \"CPU\"))\n",
    "pinn.to(device)\n",
    "\n",
    "\n",
    "input = torch.tensor(input.astype(np.float32), requires_grad=True).to(device).view(-1, 1)\n",
    "initial_vals = torch.tensor(initial_vals.astype(np.float32)).to(device).view(-1, 1)\n",
    "coefficients = torch.tensor(IRK_weights.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/IRKPINN.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep(closure)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/optim/lbfgs.py:312\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m state\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mn_iter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[39m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m orig_loss \u001b[39m=\u001b[39m closure()\n\u001b[1;32m    313\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    314\u001b[0m current_evals \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/Users/alexning/Desktop/Life/UVA/Research/Research1 (Public Github repo)/PINN 2 - Implicit Runge-Kutta/IRKPINN.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m#reset the gradient so that the previous iteration does not affect the current one\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m pinn(\u001b[39minput\u001b[39m) \u001b[39m#run the batch through the current model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m overall_loss, SSE_n, SSE_b \u001b[39m=\u001b[39m SSE(\u001b[39minput\u001b[39m, outputs, initial_vals, coefficients, pinn, device) \u001b[39m#calculate the loss\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m overall_loss\u001b[39m.\u001b[39mbackward() \u001b[39m#Using backpropagation, calculate the gradients\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alexning/Desktop/Life/UVA/Research/Research1%20%28Public%20Github%20repo%29/PINN%202%20-%20Implicit%20Runge-Kutta/IRKPINN.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#print(f\"Avg loss: {loss.item()}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env-1-ai/lib/python3.11/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[39m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[39m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[39m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39miteration over a 0-d tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing a tensor of different shape won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt change the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad() #reset the gradient so that the previous iteration does not affect the current one\n",
    "    outputs = pinn(input) #run the batch through the current model\n",
    "    \n",
    "    overall_loss, SSE_n, SSE_b = SSE(input, outputs, initial_vals, coefficients, pinn, device) #calculate the loss\n",
    "    overall_loss.backward() #Using backpropagation, calculate the gradients\n",
    "    #print(f\"Avg loss: {loss.item()}\")\n",
    "\n",
    "    global i\n",
    "    if i%100 == 0:\n",
    "        print(f\"STEP: {i} | Avg Losses | SSE_n: {SSE_n.item()} | SSE_b: {SSE_b.item()} | Total: {overall_loss.item()}\")\n",
    "    i += 1\n",
    "\n",
    "    return overall_loss\n",
    "\n",
    "optimizer.step(closure) #Using the gradients, adjust the parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-1-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
